{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker SDK Training & Hyperparameter Tuning\n",
    "\n",
    "**Lab 3 - Assignments 2 & 3 Answer Notebook**\n",
    "\n",
    "This notebook demonstrates model training and hyperparameter tuning using the SageMaker Python SDK, answering:\n",
    "- **Assignment 2**: Training with Framework Estimators\n",
    "- **Assignment 3**: Hyperparameter Tuning with the SDK\n",
    "\n",
    "**Key Benefits of SDK Approach:**\n",
    "- 80% less code compared to sagemaker-core\n",
    "- High-level ML abstractions (Estimators, Tuners, Predictors)\n",
    "- Automatic handling of AWS resource configuration\n",
    "- Clean inference with automatic serialization\n",
    "- Integrated hyperparameter tuning workflow"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:48:18.167970Z",
     "start_time": "2025-10-17T07:48:18.046276Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from aws.get_started import endpoint_name\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "We'll use our existing `CoreLabSession` for session management but switch to SageMaker SDK for ML operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:48:26.806769Z",
     "start_time": "2025-10-17T07:48:18.177462Z"
    }
   },
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "# Use our custom session for authentication and S3 management\n",
    "lab_session = CoreLabSession('pytorch', 'customer-churn',\n",
    "                            default_folder='sagemaker_sdk_notebook', \n",
    "                            create_run_folder=True,\n",
    "                             aws_profile='sagemaker-role')\n",
    "lab_session.print()\n",
    "\n",
    "# Get SageMaker session for SDK integration\n",
    "sagemaker_session = lab_session.get_sagemaker_session()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/machiel/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name machiel-crystalline to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "falling back to profile: sagemaker-role\n",
      "AWS region: eu-central-1\n",
      "Execution role arn:aws:iam::136548476532:role/service-role/AmazonSageMaker-ExecutionRole-20250902T164316\n",
      "Output bucket uri: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24\n",
      "Framework: pytorch\n",
      "Project name: customer-churn\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Assignment 2: Training with Framework Estimators\n",
    "\n",
    "This section demonstrates training with the **PyTorch Framework Estimator** with a custom XGBoost training script - the modern, flexible approach (Lab 3 Option A - Recommended).\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Using PyTorch Framework Estimator for custom training logic\n",
    "- Running XGBoost within PyTorch container (modern Python ecosystem)\n",
    "- Creating custom training scripts with SageMaker conventions\n",
    "- Passing hyperparameters as command-line arguments\n",
    "- Automatic dependency installation via requirements.txt\n",
    "\n",
    "**Key Approach:**\n",
    "- **Framework Estimator**: PyTorch (provides modern container environment)\n",
    "- **ML Library**: XGBoost (installed via requirements.txt)\n",
    "- **Training Script**: Custom `train.py` with full control over training logic\n",
    "\n",
    "**SDK vs. sagemaker-core:**\n",
    "What took 50+ lines of TrainingJob configuration becomes just a few lines with the Framework Estimator pattern, while maintaining full flexibility through custom training scripts."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:48:40.358008Z",
     "start_time": "2025-10-17T07:48:40.112932Z"
    }
   },
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Create PyTorch Framework Estimator with custom XGBoost training script\n",
    "# This uses PyTorch container for modern Python ecosystem while training with XGBoost\n",
    "my_estimator = PyTorch(\n",
    "    entry_point='train.py',           # Custom training script\n",
    "    source_dir='src/',                # Directory with train.py and requirements.txt\n",
    "    framework_version='2.6.0',          # PyTorch version (not XGBoost!)\n",
    "    py_version='py312',\n",
    "    role=lab_session.role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    output_path=lab_session.base_s3_uri,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='pytorch-xgboost-churn',\n",
    "\n",
    "    # XGBoost hyperparameters (passed to train.py as CLI arguments)\n",
    "    # Note: Use hyphens not underscores for CLI arg compatibility\n",
    "    hyperparameters={\n",
    "        'max-depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min-child-weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num-round': 100,\n",
    "        'eval-metric': 'auc'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ PyTorch Framework Estimator configured\")\n",
    "print(f\"Training will use: {my_estimator.instance_type}\")\n",
    "print(f\"Entry point: {my_estimator.entry_point}\")\n",
    "print(f\"Output location: {my_estimator.output_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch Framework Estimator configured\n",
      "Training will use: ml.m5.xlarge\n",
      "Entry point: train.py\n",
      "Output location: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T07:52:12.040641Z",
     "start_time": "2025-10-17T07:48:52.518133Z"
    }
   },
   "source": [
    "# Train the model - just one line!\n",
    "# Compare this to the complex TrainingJob.create() in sagemaker-core\n",
    "\n",
    "s3_train_path = \"s3://sagemaker-eu-central-1-136548476532/preprocessing_sdk/2025-10-15T11-26-31/customer-churn-2025-10-15T11-26-31/jobs/customer-churn-pytorch-processing-2025-10-15T11-47-44/train/\"\n",
    "\n",
    "s3_validation_path = \"s3://sagemaker-eu-central-1-136548476532/preprocessing_sdk/2025-10-15T11-26-31/customer-churn-2025-10-15T11-26-31/jobs/customer-churn-pytorch-processing-2025-10-15T11-47-44/validation/\"\n",
    "\n",
    "s3_test_path = \"s3://sagemaker-eu-central-1-136548476532/preprocessing_sdk/2025-10-15T11-26-31/customer-churn-2025-10-15T11-26-31/jobs/customer-churn-pytorch-processing-2025-10-15T11-47-44/test/\"\n",
    "\n",
    "\n",
    "my_estimator.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(f\"✅ Training completed!\")\n",
    "print(f\"Model artifacts: {my_estimator.model_data}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-xgboost-churn-2025-10-17-07-48-52-543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-17 07:48:53 Starting - Starting the training job...\n",
      "2025-10-17 07:49:18 Starting - Preparing the instances for training...\n",
      "2025-10-17 07:49:36 Downloading - Downloading input data...\n",
      "2025-10-17 07:50:02 Downloading - Downloading the training image...\n",
      "2025-10-17 07:50:53 Training - Training image download completed. Training in progress..\u001B[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001B[0m\n",
      "\u001B[34mbash: no job control in this shell\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:04,024 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:04,024 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:04,025 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:04,034 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:04,035 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:05,708 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001B[0m\n",
      "\u001B[34mCollecting xgboost==3.0.5 (from -r requirements.txt (line 1))\u001B[0m\n",
      "\u001B[34mDownloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 2)) (2.3.1)\u001B[0m\n",
      "\u001B[34mCollecting sagemaker-training>=5.1.1 (from -r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mDownloading sagemaker_training-5.1.1.tar.gz (59 kB)\u001B[0m\n",
      "\u001B[34mInstalling build dependencies: started\u001B[0m\n",
      "\u001B[34mInstalling build dependencies: finished with status 'done'\u001B[0m\n",
      "\u001B[34mGetting requirements to build wheel: started\u001B[0m\n",
      "\u001B[34mGetting requirements to build wheel: finished with status 'done'\u001B[0m\n",
      "\u001B[34mPreparing metadata (pyproject.toml): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting sagemaker-inference>=1.10.1 (from -r requirements.txt (line 4))\u001B[0m\n",
      "\u001B[34mDownloading sagemaker_inference-1.10.1.tar.gz (23 kB)\u001B[0m\n",
      "\u001B[34mInstalling build dependencies: started\u001B[0m\n",
      "\u001B[34mInstalling build dependencies: finished with status 'done'\u001B[0m\n",
      "\u001B[34mGetting requirements to build wheel: started\u001B[0m\n",
      "\u001B[34mGetting requirements to build wheel: finished with status 'done'\u001B[0m\n",
      "\u001B[34mPreparing metadata (pyproject.toml): started\u001B[0m\n",
      "\u001B[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.7.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from xgboost==3.0.5->-r requirements.txt (line 1)) (1.26.4)\u001B[0m\n",
      "\u001B[34mCollecting nvidia-nccl-cu12 (from xgboost==3.0.5->-r requirements.txt (line 1))\u001B[0m\n",
      "\u001B[34mDownloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from xgboost==3.0.5->-r requirements.txt (line 1)) (1.16.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2.9.0.post0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2025.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r requirements.txt (line 2)) (2025.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: boto3 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.39.7)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.17.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pip in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (25.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.4.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: gevent in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (25.5.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: inotify_simple==1.2.1 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.2.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: werkzeug>=0.15.5 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (3.1.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: paramiko>=2.4.2 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (3.5.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: psutil>=5.6.7 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (7.0.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: protobuf>=5.28.1 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (6.31.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: botocore>=1.31.57 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.39.7)\u001B[0m\n",
      "\u001B[34mCollecting retrying>=1.3.3 (from sagemaker-training>=5.1.1->-r requirements.txt (line 3))\u001B[0m\n",
      "\u001B[34mDownloading retrying-1.3.7-py3-none-any.whl.metadata (7.3 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 5)) (1.5.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r requirements.txt (line 5)) (3.6.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from boto3->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/site-packages (from boto3->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (0.13.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore>=1.31.57->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (2.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: bcrypt>=3.2 in /usr/local/lib/python3.12/site-packages (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (4.3.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.12/site-packages (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (45.0.5)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pynacl>=1.5 in /usr/local/lib/python3.12/site-packages (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.5.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cffi>=1.14 in /usr/local/lib/python3.12/site-packages (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (1.17.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=3.3->paramiko>=2.4.2->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (2.22)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=0.15.5->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (3.0.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.12/site-packages (from gevent->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (3.2.3)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zope.event in /usr/local/lib/python3.12/site-packages (from gevent->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (5.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: zope.interface in /usr/local/lib/python3.12/site-packages (from gevent->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (7.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (from zope.event->gevent->sagemaker-training>=5.1.1->-r requirements.txt (line 3)) (80.9.0)\u001B[0m\n",
      "\u001B[34mDownloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.9/94.9 MB 128.9 MB/s  0:00:00\u001B[0m\n",
      "\u001B[34mDownloading retrying-1.3.7-py3-none-any.whl (11 kB)\u001B[0m\n",
      "\u001B[34mDownloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl (295.9 MB)\u001B[0m\n",
      "\u001B[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.9/295.9 MB 126.4 MB/s  0:00:02\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: sagemaker-training, sagemaker-inference\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker-training (pyproject.toml): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker-training (pyproject.toml): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for sagemaker-training: filename=sagemaker_training-5.1.1-cp312-cp312-linux_x86_64.whl size=88366 sha256=0132cdcee41a851ce000d5b0413895e2a980e82876d951315bf404dc8949b3c2\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/6c/1a/dd/3623eed42223170d254d572bdd34a4f976cac5200ae2092cc6\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker-inference (pyproject.toml): started\u001B[0m\n",
      "\u001B[34mBuilding wheel for sagemaker-inference (pyproject.toml): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCreated wheel for sagemaker-inference: filename=sagemaker_inference-1.10.1-py2.py3-none-any.whl size=29838 sha256=fccda4bf400549715ed1f4eb86c3d5d802fb35aec32b248ea80e1a11db0c0b35\u001B[0m\n",
      "\u001B[34mStored in directory: /root/.cache/pip/wheels/ed/bb/c9/473181075d65511229428dce029a6d9309106c95503b3cc835\u001B[0m\n",
      "\u001B[34mSuccessfully built sagemaker-training sagemaker-inference\u001B[0m\n",
      "\u001B[34mInstalling collected packages: retrying, nvidia-nccl-cu12, xgboost, sagemaker-training, sagemaker-inference\u001B[0m\n",
      "\u001B[34mAttempting uninstall: retrying\u001B[0m\n",
      "\u001B[34mFound existing installation: retrying 1.4.0\u001B[0m\n",
      "\u001B[34mUninstalling retrying-1.4.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled retrying-1.4.0\u001B[0m\n",
      "\n",
      "2025-10-17 07:51:28 Uploading - Uploading generated training model\u001B[34mAttempting uninstall: sagemaker-training\u001B[0m\n",
      "\u001B[34mFound existing installation: sagemaker_training 5.0.0\u001B[0m\n",
      "\u001B[34mUninstalling sagemaker_training-5.0.0:\u001B[0m\n",
      "\u001B[34mSuccessfully uninstalled sagemaker_training-5.0.0\u001B[0m\n",
      "\u001B[34mSuccessfully installed nvidia-nccl-cu12-2.28.3 retrying-1.3.7 sagemaker-inference-1.10.1 sagemaker-training-5.1.1 xgboost-3.0.5\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,004 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,005 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,006 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,016 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,017 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,026 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,027 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,036 sagemaker-training-toolkit INFO     Invoking user script\u001B[0m\n",
      "\u001B[34mTraining Env:\u001B[0m\n",
      "\u001B[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": 0.2,\n",
      "        \"eval-metric\": \"auc\",\n",
      "        \"gamma\": 4,\n",
      "        \"max-depth\": 5,\n",
      "        \"min-child-weight\": 6,\n",
      "        \"num-round\": 100,\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"subsample\": 0.8\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.m5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-xgboost-churn-2025-10-17-07-48-52-543\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-136548476532/pytorch-xgboost-churn-2025-10-17-07-48-52-543/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001B[0m\n",
      "\u001B[34m}\u001B[0m\n",
      "\u001B[34mEnvironment variables:\u001B[0m\n",
      "\u001B[34mSM_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_NETWORK_INTERFACE_NAME=eth0\u001B[0m\n",
      "\u001B[34mSM_HPS={\"eta\":0.2,\"eval-metric\":\"auc\",\"gamma\":4,\"max-depth\":5,\"min-child-weight\":6,\"num-round\":100,\"objective\":\"binary:logistic\",\"subsample\":0.8}\u001B[0m\n",
      "\u001B[34mSM_USER_ENTRY_POINT=train.py\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_PARAMS={}\u001B[0m\n",
      "\u001B[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001B[0m\n",
      "\u001B[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001B[0m\n",
      "\u001B[34mSM_CHANNELS=[\"train\",\"validation\"]\u001B[0m\n",
      "\u001B[34mSM_CURRENT_HOST=algo-1\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_TYPE=ml.m5.xlarge\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001B[0m\n",
      "\u001B[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001B[0m\n",
      "\u001B[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}}\u001B[0m\n",
      "\u001B[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001B[0m\n",
      "\u001B[34mSM_IS_HETERO=false\u001B[0m\n",
      "\u001B[34mSM_MODULE_NAME=train\u001B[0m\n",
      "\u001B[34mSM_LOG_LEVEL=20\u001B[0m\n",
      "\u001B[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001B[0m\n",
      "\u001B[34mSM_INPUT_DIR=/opt/ml/input\u001B[0m\n",
      "\u001B[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_DIR=/opt/ml/output\u001B[0m\n",
      "\u001B[34mSM_NUM_CPUS=4\u001B[0m\n",
      "\u001B[34mSM_NUM_GPUS=0\u001B[0m\n",
      "\u001B[34mSM_NUM_NEURONS=0\u001B[0m\n",
      "\u001B[34mSM_MODEL_DIR=/opt/ml/model\u001B[0m\n",
      "\u001B[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-136548476532/pytorch-xgboost-churn-2025-10-17-07-48-52-543/source/sourcedir.tar.gz\u001B[0m\n",
      "\u001B[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.m5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":0.2,\"eval-metric\":\"auc\",\"gamma\":4,\"max-depth\":5,\"min-child-weight\":6,\"num-round\":100,\"objective\":\"binary:logistic\",\"subsample\":0.8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"pytorch-xgboost-churn-2025-10-17-07-48-52-543\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-136548476532/pytorch-xgboost-churn-2025-10-17-07-48-52-543/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"train.py\"}\u001B[0m\n",
      "\u001B[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--eval-metric\",\"auc\",\"--gamma\",\"4\",\"--max-depth\",\"5\",\"--min-child-weight\",\"6\",\"--num-round\",\"100\",\"--objective\",\"binary:logistic\",\"--subsample\",\"0.8\"]\u001B[0m\n",
      "\u001B[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001B[0m\n",
      "\u001B[34mSM_HP_ETA=0.2\u001B[0m\n",
      "\u001B[34mSM_HP_EVAL-METRIC=auc\u001B[0m\n",
      "\u001B[34mSM_HP_GAMMA=4\u001B[0m\n",
      "\u001B[34mSM_HP_MAX-DEPTH=5\u001B[0m\n",
      "\u001B[34mSM_HP_MIN-CHILD-WEIGHT=6\u001B[0m\n",
      "\u001B[34mSM_HP_NUM-ROUND=100\u001B[0m\n",
      "\u001B[34mSM_HP_OBJECTIVE=binary:logistic\u001B[0m\n",
      "\u001B[34mSM_HP_SUBSAMPLE=0.8\u001B[0m\n",
      "\u001B[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python312.zip:/usr/local/lib/python3.12:/usr/local/lib/python3.12/lib-dynload:/usr/local/lib/python3.12/site-packages\u001B[0m\n",
      "\u001B[34mInvoking script with the following command:\u001B[0m\n",
      "\u001B[34m/usr/local/bin/python train.py --eta 0.2 --eval-metric auc --gamma 4 --max-depth 5 --min-child-weight 6 --num-round 100 --objective binary:logistic --subsample 0.8\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,037 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:21,037 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34mSAGEMAKER ENVIRONMENT\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34m{'additional_framework_parameters': {}, 'channel_input_dirs': {'train': '/opt/ml/input/data/train', 'validation': '/opt/ml/input/data/validation'}, 'current_host': 'algo-1', 'current_instance_group': 'homogeneousCluster', 'current_instance_group_hosts': ['algo-1'], 'current_instance_type': 'ml.m5.xlarge', 'distribution_hosts': [], 'distribution_instance_groups': [], 'framework_module': 'sagemaker_pytorch_container.training:main', 'hosts': ['algo-1'], 'hyperparameters': {'eta': 0.2, 'eval-metric': 'auc', 'gamma': 4, 'max-depth': 5, 'min-child-weight': 6, 'num-round': 100, 'objective': 'binary:logistic', 'subsample': 0.8}, 'input_config_dir': '/opt/ml/input/config', 'input_data_config': {'train': {'TrainingInputMode': 'File', 'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None'}, 'validation': {'TrainingInputMode': 'File', 'S3DistributionType': 'FullyReplicated', 'RecordWrapperType': 'None'}}, 'input_dir': '/opt/ml/input', 'instance_groups': ['homogeneousCluster'], 'instance_groups_dict': {'homogeneousCluster': {'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}}, 'is_hetero': False, 'is_master': True, 'is_modelparallel_enabled': None, 'is_smddpmprun_installed': False, 'is_smddprun_installed': False, 'job_name': 'pytorch-xgboost-churn-2025-10-17-07-48-52-543', 'log_level': 20, 'master_hostname': 'algo-1', 'model_dir': '/opt/ml/model', 'module_dir': 's3://sagemaker-eu-central-1-136548476532/pytorch-xgboost-churn-2025-10-17-07-48-52-543/source/sourcedir.tar.gz', 'module_name': 'train', 'network_interface_name': 'eth0', 'num_cpus': 4, 'num_gpus': 0, 'num_neurons': 0, 'output_data_dir': '/opt/ml/output/data', 'output_dir': '/opt/ml/output', 'output_intermediate_dir': '/opt/ml/output/intermediate', 'resource_config': {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}, 'topology': None, 'user_entry_point': 'train.py'}\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34mTRAINING CONFIGURATION\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34mModel directory: /opt/ml/model\u001B[0m\n",
      "\u001B[34mTrain data: /opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mValidation data: /opt/ml/input/data/validation\u001B[0m\n",
      "\u001B[34mSystem Info:\n",
      "  GPUs: 0, CPUs: 4\n",
      "  Hosts: ['algo-1'], Current: algo-1\u001B[0m\n",
      "\u001B[34mHyperparameters:\n",
      "  max_depth: 5\n",
      "  eta: 0.2\n",
      "  gamma: 4.0\n",
      "  min_child_weight: 6.0\n",
      "  subsample: 0.8\n",
      "  objective: binary:logistic\n",
      "  num_round: 100\n",
      "  eval_metric: auc\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34m📥 Loading data...\u001B[0m\n",
      "\u001B[34mLoading 1 file(s) from /opt/ml/input/data/train\u001B[0m\n",
      "\u001B[34mLoaded /opt/ml/input/data/train/train.csv: (3350, 100)\u001B[0m\n",
      "\u001B[34mTotal samples: 3350, Features: 99\u001B[0m\n",
      "\u001B[34mLoading 1 file(s) from /opt/ml/input/data/validation\u001B[0m\n",
      "\u001B[34mLoaded /opt/ml/input/data/validation/validation.csv: (1105, 100)\u001B[0m\n",
      "\u001B[34mTotal samples: 1105, Features: 99\u001B[0m\n",
      "\u001B[34m🔨 Creating DMatrix...\u001B[0m\n",
      "\u001B[34m🚀 Starting training...\u001B[0m\n",
      "\u001B[34mTraining for 100 rounds with early stopping\u001B[0m\n",
      "\u001B[34m[0]#011train-auc:0.92526#011validation-auc:0.91339\u001B[0m\n",
      "\u001B[34m[10]#011train-auc:0.97897#011validation-auc:0.97171\u001B[0m\n",
      "\u001B[34m[20]#011train-auc:0.98480#011validation-auc:0.97750\u001B[0m\n",
      "\u001B[34m[30]#011train-auc:0.98535#011validation-auc:0.97796\u001B[0m\n",
      "\u001B[34m[40]#011train-auc:0.98551#011validation-auc:0.97818\u001B[0m\n",
      "\u001B[34m[48]#011train-auc:0.98550#011validation-auc:0.97816\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34mTRAINING COMPLETED\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34mFinal train auc: 0.9855\u001B[0m\n",
      "\u001B[34mFinal validation auc: 0.9782\u001B[0m\n",
      "\u001B[34mBest iteration: 38\u001B[0m\n",
      "\u001B[34mBest score: 0.9782\u001B[0m\n",
      "\u001B[34mvalidation-auc:0.9782\u001B[0m\n",
      "\u001B[34mtrain-auc:0.9855\u001B[0m\n",
      "\u001B[34m💾 Saving model...\u001B[0m\n",
      "\u001B[34m/opt/ml/code/train.py:173: UserWarning: [07:51:22] WARNING: /workspace/src/c_api/c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  model.save_model(model_path)\u001B[0m\n",
      "\u001B[34m✅ Model saved to /opt/ml/model/xgboost-model.bin\u001B[0m\n",
      "\u001B[34m✅ Metadata saved to /opt/ml/model/metadata.json\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34m🎉 TRAINING PIPELINE COMPLETED SUCCESSFULLY\u001B[0m\n",
      "\u001B[34m============================================================\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:22,585 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:22,585 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001B[0m\n",
      "\u001B[34m2025-10-17 07:51:22,586 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001B[0m\n",
      "\n",
      "2025-10-17 07:51:41 Completed - Training job completed\n",
      "Training seconds: 125\n",
      "Billable seconds: 125\n",
      "✅ Training completed!\n",
      "Model artifacts: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-07-48-52-543/output/model.tar.gz\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Assignment 3: Hyperparameter Tuning with the SDK\n",
    "\n",
    "This section demonstrates automated hyperparameter optimization using the **SageMaker SDK's HyperparameterTuner**.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Defining hyperparameter search spaces with typed parameters\n",
    "- Configuring Bayesian optimization strategy\n",
    "- Running parallel tuning jobs with resource management\n",
    "- Analyzing tuning results and selecting best models\n",
    "\n",
    "**SDK vs. sagemaker-core:**\n",
    "The HyperparameterTuner class makes tuning much more intuitive compared to the complex HyperParameterTuningJobConfig shapes from Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner,\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges - much cleaner than sagemaker-core!\n",
    "# Note: Use hyphens to match CLI argument format in train.py\n",
    "hyperparameter_ranges = {\n",
    "    'max-depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min-child-weight': ContinuousParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'num-round': IntegerParameter(50, 200)\n",
    "}\n",
    "\n",
    "# Create tuner with metric definitions\n",
    "# IMPORTANT: metric_definitions is required for framework estimators (not built-in algorithms)\n",
    "# since SageMaker doesn't know how to parse metrics from custom training scripts\n",
    "tuner = HyperparameterTuner(\n",
    "    my_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'validation:auc', 'Regex': 'validation-auc:([0-9\\\\.]+)'},\n",
    "        {'Name': 'train:auc', 'Regex': 'train-auc:([0-9\\\\.]+)'}\n",
    "    ],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    "    base_tuning_job_name='pytorch-xgboost-tuning'\n",
    ")\n",
    "\n",
    "print(\"✅ Hyperparameter tuner configured\")\n",
    "print(f\"Will run {tuner.max_jobs} tuning jobs\")\n",
    "print(f\"Optimizing: {tuner.objective_metric_name}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Start tuning - one line vs complex sagemaker-core setup!\n",
    "tuner.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(\"✅ Hyperparameter tuning completed!\")\n",
    "\n",
    "# Get best training job details using HyperparameterTuningJobAnalytics\n",
    "# Note: best_training_job() returns a string (job name), not a dictionary\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name)\n",
    "full_df = tuner_analytics.dataframe()\n",
    "\n",
    "# Get best training job (highest validation:auc)\n",
    "best_job_row = full_df.sort_values('FinalObjectiveValue', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"\\nBest job: {best_job_row['TrainingJobName']}\")\n",
    "print(f\"Best AUC: {best_job_row['FinalObjectiveValue']:.4f}\")\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key in hyperparameter_ranges.keys():\n",
    "    print(f\"  {key}: {best_job_row[key]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Now we'll deploy the model using different strategies: provisioned endpoints, serverless endpoints, and batch transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorchModel with Custom Inference Handler\n",
    "\n",
    "Before deploying, we create a `PyTorchModel` with our custom `inference.py` handler. This model will be reused for all deployment types (provisioned endpoint, serverless endpoint, and batch transform), ensuring consistent inference behavior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:37:32.667612Z",
     "start_time": "2025-10-17T12:37:32.308722Z"
    }
   },
   "source": [
    "final_estimator = tuner.best_estimator() if 'tuner' in locals() else my_estimator\n",
    "\n",
    "# Create PyTorchModel with custom inference handler\n",
    "# This will be reused for all deployments (endpoints and batch transform)\n",
    "\n",
    "churn_model = final_estimator.create_model(source_dir=final_estimator.source_dir, entry_point='inference.py')\n",
    "\n",
    "print(\"✅ Churn model created with custom inference handler\")\n",
    "print(f\"Model data: {churn_model.model_data}\")\n",
    "print(f\"Entry point: {churn_model.entry_point}\")\n",
    "print(f\"Model name: {churn_model.name}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Churn model created with custom inference handler\n",
      "Model data: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-07-48-52-543/output/model.tar.gz\n",
      "Entry point: inference.py\n",
      "Model name: pytorch-xgboost-churn-2025-10-17-12-37-32-511\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:40:08.305628Z",
     "start_time": "2025-10-17T12:37:35.309494Z"
    }
   },
   "source": [
    "from sagemaker import Predictor\n",
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Deploy serverless endpoint using the same PyTorchModel\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,\n",
    "    max_concurrency=10,\n",
    ")\n",
    "\n",
    "try:\n",
    "    p = Predictor(endpoint_name=lab_session.serverless_endpoint_name)\n",
    "    p.delete_endpoint()\n",
    "    print(\"Removed previous endpoint (config)\")\n",
    "except Exception as e:\n",
    "    print(\"No previous endpoint found (\", e, \")\")\n",
    "    pass\n",
    "\n",
    "serverless_predictor = churn_model.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=lab_session.serverless_endpoint_name,\n",
    ")\n",
    "\n",
    "print(f\"✅ Serverless model deployed: {serverless_predictor.endpoint_name}\")\n",
    "print(f\"Memory: {serverless_config.memory_size_in_mb}MB\")\n",
    "print(f\"Max concurrency: {serverless_config.max_concurrency}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: customer-churn-pytorch-serverless-endpoint\n",
      "INFO:sagemaker:Deleting endpoint with name: customer-churn-pytorch-serverless-endpoint\n",
      "INFO:sagemaker.image_uris:Defaulting to CPU type when using serverless inference\n",
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-07-48-52-543/output/model.tar.gz), script artifact (src/), and dependencies ([]) into single tar.gz file located at s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-12-37-32-511/model.tar.gz. This may take some time depending on model size...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed previous endpoint (config)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: pytorch-xgboost-churn-2025-10-17-12-37-32-511\n",
      "INFO:sagemaker:Creating endpoint-config with name customer-churn-pytorch-serverless-endpoint\n",
      "INFO:sagemaker:Creating endpoint with name customer-churn-pytorch-serverless-endpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!✅ Serverless model deployed: customer-churn-pytorch-serverless-endpoint\n",
      "Memory: 2048MB\n",
      "Max concurrency: 10\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform\n",
    "\n",
    "The SageMaker SDK also simplifies batch inference.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Reusing PyTorchModel**: We use the same `PyTorchModel` created earlier that includes our custom `inference.py` handler. This ensures consistent inference behavior across endpoints and batch transform.\n",
    "\n",
    "2. **Custom Inference Handler**: The `inference.py` script handles XGBoost models in the PyTorch container with four functions:\n",
    "   - `model_fn()`: Load the XGBoost model from disk\n",
    "   - `input_fn()`: Parse CSV input into XGBoost DMatrix (handles structured arrays)\n",
    "   - `predict_fn()`: Run inference with the model\n",
    "   - `output_fn()`: Format predictions as CSV output"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:46:05.385561Z",
     "start_time": "2025-10-17T12:40:08.330347Z"
    }
   },
   "source": [
    "# Create transformer from the PyTorchModel with custom inference handler\n",
    "# Uses the same pytorch_model we created earlier with inference.py\n",
    "\n",
    "transformer = churn_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=lab_session.transform_output_s3_uri,\n",
    ")\n",
    "\n",
    "# Run batch transform\n",
    "transformer.transform(\n",
    "    data=s3_test_path,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "print(f\"✅ Batch transform completed!\")\n",
    "print(f\"Results saved to: {transformer.output_path}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-07-48-52-543/output/model.tar.gz), script artifact (src/), and dependencies ([]) into single tar.gz file located at s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/pytorch-xgboost-churn-2025-10-17-12-37-32-511/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: pytorch-xgboost-churn-2025-10-17-12-37-32-511\n",
      "WARNING:sagemaker:Using already existing model: pytorch-xgboost-churn-2025-10-17-12-37-32-511\n",
      "INFO:sagemaker:Creating transform job with name: pytorch-xgboost-churn-2025-10-17-12-37--2025-10-17-12-40-09-499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\u001B[34m/usr/local/lib/python3.12/site-packages/sagemaker_pytorch_serving_container/torchserve.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\u001B[0m\n",
      "\u001B[34mCollecting xgboost==3.0.5 (from -r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pandas>=1.5.0 in /usr/local/lib/python3.12/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (2.3.1)\u001B[0m\n",
      "\u001B[34mCollecting sagemaker-training>=5.1.1 (from -r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading sagemaker_training-5.1.1.tar.gz (59 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting sagemaker-inference>=1.10.1 (from -r /opt/ml/model/code/requirements.txt (line 4))\n",
      "  Downloading sagemaker_inference-1.10.1.tar.gz (23 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.12/site-packages (from -r /opt/ml/model/code/requirements.txt (line 5)) (1.7.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: numpy in /usr/local/lib/python3.12/site-packages (from xgboost==3.0.5->-r /opt/ml/model/code/requirements.txt (line 1)) (2.2.6)\u001B[0m\n",
      "\u001B[34mCollecting nvidia-nccl-cu12 (from xgboost==3.0.5->-r /opt/ml/model/code/requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl.metadata (2.0 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: scipy in /usr/local/lib/python3.12/site-packages (from xgboost==3.0.5->-r /opt/ml/model/code/requirements.txt (line 1)) (1.16.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.9.0.post0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2025.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas>=1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2025.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: boto3 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.39.6)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: six in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.17.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pip in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (25.2)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.4.0)\u001B[0m\n",
      "\u001B[34mCollecting gevent (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\u001B[0m\n",
      "\u001B[34mCollecting inotify_simple==1.2.1 (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading inotify_simple-1.2.1.tar.gz (7.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001B[0m\n",
      "\u001B[34mCollecting werkzeug>=0.15.5 (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\u001B[0m\n",
      "\u001B[34mCollecting paramiko>=2.4.2 (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading paramiko-4.0.0-py3-none-any.whl.metadata (3.9 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: psutil>=5.6.7 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (5.9.8)\u001B[0m\n",
      "\u001B[34mCollecting protobuf>=5.28.1 (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: botocore>=1.31.57 in /usr/local/lib/python3.12/site-packages (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.39.6)\u001B[0m\n",
      "\u001B[34mCollecting retrying>=1.3.3 (from sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading retrying-1.3.7-py3-none-any.whl.metadata (7.3 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r /opt/ml/model/code/requirements.txt (line 5)) (1.5.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/site-packages (from scikit-learn>=1.0.0->-r /opt/ml/model/code/requirements.txt (line 5)) (3.6.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/site-packages (from boto3->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.0.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.12/site-packages (from boto3->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (0.13.0)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/site-packages (from botocore>=1.31.57->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (2.5.0)\u001B[0m\n",
      "\u001B[34mCollecting bcrypt>=3.2 (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.12/site-packages (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (45.0.5)\u001B[0m\n",
      "\u001B[34mCollecting invoke>=2.0 (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\u001B[0m\n",
      "\u001B[34mCollecting pynacl>=1.5 (from paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl.metadata (9.4 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: cffi>=1.14 in /usr/local/lib/python3.12/site-packages (from cryptography>=3.3->paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (1.17.1)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: pycparser in /usr/local/lib/python3.12/site-packages (from cffi>=1.14->cryptography>=3.3->paramiko>=2.4.2->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (2.22)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/site-packages (from werkzeug>=0.15.5->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (3.0.2)\u001B[0m\n",
      "\u001B[34mCollecting greenlet>=3.2.2 (from gevent->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\u001B[0m\n",
      "\u001B[34mCollecting zope.event (from gevent->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading zope_event-6.0-py3-none-any.whl.metadata (5.1 kB)\u001B[0m\n",
      "\u001B[34mCollecting zope.interface (from gevent->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3))\n",
      "  Downloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (44 kB)\u001B[0m\n",
      "\u001B[34mRequirement already satisfied: setuptools>=75.8.2 in /usr/local/lib/python3.12/site-packages (from zope.event->gevent->sagemaker-training>=5.1.1->-r /opt/ml/model/code/requirements.txt (line 3)) (80.9.0)\u001B[0m\n",
      "\u001B[34mDownloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\u001B[0m\n",
      "\u001B[34m   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.9/94.9 MB 98.5 MB/s  0:00:00\u001B[0m\n",
      "\u001B[34mDownloading retrying-1.3.7-py3-none-any.whl (11 kB)\u001B[0m\n",
      "\u001B[34mDownloading paramiko-4.0.0-py3-none-any.whl (223 kB)\u001B[0m\n",
      "\u001B[34mDownloading bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\u001B[0m\n",
      "\u001B[34mDownloading invoke-2.2.1-py3-none-any.whl (160 kB)\u001B[0m\n",
      "\u001B[34mDownloading protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\u001B[0m\n",
      "\u001B[34mDownloading pynacl-1.6.0-cp38-abi3-manylinux_2_34_x86_64.whl (1.4 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 85.6 MB/s  0:00:00\u001B[0m\n",
      "\u001B[34mDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\u001B[0m\n",
      "\u001B[34mDownloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.1 MB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 105.0 MB/s  0:00:00\u001B[0m\n",
      "\u001B[34mDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
      "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 607.6/607.6 kB 54.1 MB/s  0:00:00\u001B[0m\n",
      "\u001B[34mDownloading nvidia_nccl_cu12-2.28.3-py3-none-manylinux_2_18_x86_64.whl (295.9 MB)\u001B[0m\n",
      "\u001B[34m   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.9/295.9 MB 79.9 MB/s  0:00:03\u001B[0m\n",
      "\u001B[34mDownloading zope_event-6.0-py3-none-any.whl (6.4 kB)\u001B[0m\n",
      "\u001B[34mDownloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\u001B[0m\n",
      "\u001B[34mBuilding wheels for collected packages: sagemaker-training, inotify_simple, sagemaker-inference\n",
      "  DEPRECATION: Building 'sagemaker-training' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sagemaker-training'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  Building wheel for sagemaker-training (setup.py): started\u001B[0m\n",
      "\u001B[34m  Building wheel for sagemaker-training (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-training: filename=sagemaker_training-5.1.1-cp312-cp312-linux_x86_64.whl size=88366 sha256=7039ceaee4886ffccfa8a68510b8b564f7cbb6cc3733f9562bb898c9689fda20\n",
      "  Stored in directory: /root/.cache/pip/wheels/6c/1a/dd/3623eed42223170d254d572bdd34a4f976cac5200ae2092cc6\n",
      "  DEPRECATION: Building 'inotify_simple' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'inotify_simple'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  Building wheel for inotify_simple (setup.py): started\n",
      "  Building wheel for inotify_simple (setup.py): finished with status 'done'\n",
      "  Created wheel for inotify_simple: filename=inotify_simple-1.2.1-py3-none-any.whl size=8262 sha256=98b10f77e095b1f8b96c8774bba73c0d81de23becd5c07174c7086573d651821\n",
      "  Stored in directory: /root/.cache/pip/wheels/2b/31/63/30752e25500afc7fb86d71e6d8faeae8f1a74407e7b893f40a\n",
      "  DEPRECATION: Building 'sagemaker-inference' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sagemaker-inference'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n",
      "  Building wheel for sagemaker-inference (setup.py): started\u001B[0m\n",
      "\u001B[34m  Building wheel for sagemaker-inference (setup.py): finished with status 'done'\n",
      "  Created wheel for sagemaker-inference: filename=sagemaker_inference-1.10.1-py2.py3-none-any.whl size=29838 sha256=5baf4cac42d8f10a7e4104b73a38fc32716d9150bd7a20eb4d98c6af3e777c0f\n",
      "  Stored in directory: /root/.cache/pip/wheels/ed/bb/c9/473181075d65511229428dce029a6d9309106c95503b3cc835\u001B[0m\n",
      "\u001B[34mSuccessfully built sagemaker-training inotify_simple sagemaker-inference\u001B[0m\n",
      "\u001B[34mInstalling collected packages: retrying, inotify_simple, zope.interface, zope.event, werkzeug, protobuf, nvidia-nccl-cu12, invoke, greenlet, bcrypt, xgboost, pynacl, gevent, paramiko, sagemaker-training, sagemaker-inference\n",
      "  Attempting uninstall: retrying\n",
      "    Found existing installation: retrying 1.4.0\n",
      "    Uninstalling retrying-1.4.0:\n",
      "      Successfully uninstalled retrying-1.4.0\u001B[0m\n",
      "\u001B[34mSuccessfully installed bcrypt-5.0.0 gevent-25.9.1 greenlet-3.2.4 inotify_simple-1.2.1 invoke-2.2.1 nvidia-nccl-cu12-2.28.3 paramiko-4.0.0 protobuf-6.33.0 pynacl-1.6.0 retrying-1.3.7 sagemaker-inference-1.10.1 sagemaker-training-5.1.1 werkzeug-3.1.3 xgboost-3.0.5 zope.event-6.0 zope.interface-8.0.1\u001B[0m\n",
      "\u001B[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\n",
      "\u001B[34m['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/usr/local/lib/python3.12/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\u001B[0m\n",
      "\u001B[34mWarning: TorchServe is using non-default JVM parameters: -XX:-UseContainerSupport\u001B[0m\n",
      "\u001B[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001B[0m\n",
      "\u001B[34mnvidia-smi not available or failed: Cannot run program \"nvidia-smi\": error=2, No such file or directory\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,702 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,718 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,784 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,877 [INFO ] main org.pytorch.serve.ModelServer - \u001B[0m\n",
      "\u001B[34mTorchserve version: 0.12.0\u001B[0m\n",
      "\u001B[34mTS Home: /usr/local/lib/python3.12/site-packages\u001B[0m\n",
      "\u001B[34mCurrent directory: /\u001B[0m\n",
      "\u001B[34mTemp directory: /tmp\u001B[0m\n",
      "\u001B[34mMetrics config path: /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml\u001B[0m\n",
      "\u001B[34mNumber of GPUs: 0\u001B[0m\n",
      "\u001B[34mNumber of CPUs: 4\u001B[0m\n",
      "\u001B[34mMax heap size: 3934 M\u001B[0m\n",
      "\u001B[34mPython executable: /usr/local/bin/python\u001B[0m\n",
      "\u001B[34mConfig file: /etc/sagemaker-ts.properties\u001B[0m\n",
      "\u001B[34mInference address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mManagement address: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34mMetrics address: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mInitial Models: model=/opt/ml/model\u001B[0m\n",
      "\u001B[34mLog dir: /logs\u001B[0m\n",
      "\u001B[34mMetrics dir: /logs\u001B[0m\n",
      "\u001B[34mNetty threads: 0\u001B[0m\n",
      "\u001B[34mNetty client threads: 0\u001B[0m\n",
      "\u001B[34mDefault workers per model: 4\u001B[0m\n",
      "\u001B[34mBlacklist Regex: N/A\u001B[0m\n",
      "\u001B[34mMaximum Response Size: 6553500\u001B[0m\n",
      "\u001B[34mMaximum Request Size: 6553500\u001B[0m\n",
      "\u001B[34mLimit Maximum Image Pixels: true\u001B[0m\n",
      "\u001B[34mPrefer direct buffer: false\u001B[0m\n",
      "\u001B[34mAllowed Urls: [file://.*|http(s)?://.*]\u001B[0m\n",
      "\u001B[34mCustom python dependency for model allowed: false\u001B[0m\n",
      "\u001B[34mEnable metrics API: true\u001B[0m\n",
      "\u001B[34mMetrics mode: LOG\u001B[0m\n",
      "\u001B[34mDisable system metrics: false\u001B[0m\n",
      "\u001B[34mWorkflow Store: /.sagemaker/ts/models\u001B[0m\n",
      "\u001B[34mCPP log config: N/A\u001B[0m\n",
      "\u001B[34mModel config: N/A\u001B[0m\n",
      "\u001B[34mSystem metrics command: default\u001B[0m\n",
      "\u001B[34mModel API enabled: false\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,887 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,909 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,918 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createTempDir /tmp/models/54603bde435843aab9a19e8d5a472c3b\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,919 [INFO ] main org.pytorch.serve.archive.model.ModelArchive - createSymbolicDir /tmp/models/54603bde435843aab9a19e8d5a472c3b/model\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,920 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,921 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,923 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:34,944 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,100 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,101 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,112 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001B[0m\n",
      "\u001B[34mModel server started.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,840 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,942 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,945 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:45.69369888305664|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,946 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:10.171432495117188|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,947 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:18.2|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,948 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:14118.54296875|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,949 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1288.65234375|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:35,950 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:10.3|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705135\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:36,512 [INFO ] pool-2-thread-5 ACCESS_LOG - /169.254.255.130:57516 \"GET /ping HTTP/1.1\" 200 17\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:36,514 [INFO ] pool-2-thread-5 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705136\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,900 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9001, pid=74\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,901 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,926 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,927 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]74\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,928 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,930 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.12.8\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,933 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9001\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,950 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9001.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,952 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9003, pid=73\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,953 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,967 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1760705137967\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,976 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,982 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]73\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,984 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,984 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9003\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:37,986 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.12.8\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,000 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9003.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,011 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1760705138011\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,017 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,043 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:46458 \"GET /execution-parameters HTTP/1.1\" 404 1\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,057 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705138\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,076 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,089 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=71\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,091 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,104 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,106 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]71\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,106 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,107 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,109 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.12.8\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,110 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,111 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1760705138111\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,121 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9002, pid=72\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,122 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,140 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,166 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.12/site-packages/ts/configs/metrics.yaml.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,167 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]72\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,168 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,168 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9002\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,171 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.12.8\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,202 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9002.\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,203 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1760705138203\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:38,242 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,986 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1875\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,987 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5050.0|#WorkerName:W-9000-model_1.0,Level:Host|#hostname:b90ff2774b22,timestamp:1760705139\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,987 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:1.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705139\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,989 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1977\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,989 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5050.0|#WorkerName:W-9003-model_1.0,Level:Host|#hostname:b90ff2774b22,timestamp:1760705139\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:39,990 [INFO ] W-9003-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:16.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705139\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,005 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 2038\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,007 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5067.0|#WorkerName:W-9001-model_1.0,Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,008 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:12.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,045 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1842\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,045 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerLoadTime.Milliseconds:5106.0|#WorkerName:W-9002-model_1.0,Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,045 [INFO ] W-9002-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:0.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,057 [INFO ] epollEventLoopGroup-3-3 TS_METRICS - ts_inference_requests_total.Count:1.0|#model_name:model,model_version:default|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,059 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1760705140058\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,063 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,116 [INFO ] W-9000-model_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - result=[METRICS]PredictionTime.Milliseconds:53.15|#ModelName:model,Level:Model|#type:GAUGE|#hostname:b90ff2774b22,1760705140,3951314f-d25a-48d6-88ce-1f96c7d36ade, pattern=[METRICS]\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,117 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Sending response for jobId 3951314f-d25a-48d6-88ce-1f96c7d36ade\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,117 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.ms:53.15|#ModelName:model,Level:Model|#hostname:b90ff2774b22,requestID:3951314f-d25a-48d6-88ce-1f96c7d36ade,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,118 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:46468 \"POST /invocations HTTP/1.1\" 200 64\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,118 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,119 [INFO ] W-9000-model_1.0 TS_METRICS - ts_inference_latency_microseconds.Microseconds:59745.941|#model_name:model,model_version:default|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,119 [INFO ] W-9000-model_1.0 TS_METRICS - ts_queue_latency_microseconds.Microseconds:206.78|#model_name:model,model_version:default|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,119 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.Milliseconds:0.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,120 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 58\u001B[0m\n",
      "\u001B[34m2025-10-17T12:45:40,120 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.Milliseconds:4.0|#Level:Host|#hostname:b90ff2774b22,timestamp:1760705140\u001B[0m\n",
      "\n",
      "\u001B[32m2025-10-17T12:45:39.981:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001B[0m\n",
      "✅ Batch transform completed!\n",
      "Results saved to: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24/transform_output\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Inference\n",
    "\n",
    "This is where the SageMaker SDK really shines - compare this clean interface to the fiddly `invoke()` + `read()` + `decode()` + `split()` in sagemaker-core!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T12:58:16.275140Z",
     "start_time": "2025-10-17T12:57:28.532162Z"
    }
   },
   "source": [
    "from io import StringIO\n",
    "from sagemaker.s3 import S3Downloader\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "test_features = pd.read_csv(StringIO(S3Downloader.read_file(s3_test_path + \"test.csv\")))\n",
    "# pd.read_csv(s3_train_path)\n",
    "# Test both endpoints with clean interface - no more fiddly response parsing!\n",
    "sample_data = test_features.head(10).values\n",
    "\n",
    "# Serverless endpoint  \n",
    "print(\"☁️  SERVERLESS ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "serverless_predictions = serverless_predictor.predict(sample_data)  # Also clean!\n",
    "serverless_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(serverless_predictions).shape}\")\n",
    "print(f\"   Latency: {serverless_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {serverless_predictions[:4]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compare results\n",
    "# predictions_match = np.allclose(provisioned_predictions, serverless_predictions, rtol=1e-5)\n",
    "# print(f\"✅ Predictions match: {predictions_match}\")\n",
    "# print(f\"📊 Latency difference: {abs(serverless_latency - provisioned_latency):.1f}ms\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "☁️  SERVERLESS ENDPOINT:\n",
      "   Predictions shape: (10,)\n",
      "   Latency: 47533.3ms\n",
      "   Sample predictions: [0.11478397 0.73954403 0.65736735 0.90863174]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The SageMaker SDK also makes cleanup simpler with built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T13:00:36.988389Z",
     "start_time": "2025-10-17T13:00:36.451680Z"
    }
   },
   "source": [
    "# Clean up resources - comprehensive cleanup including configurations and models\n",
    "\n",
    "print(\"🧹 Cleaning up resources...\")\n",
    "\n",
    "# Import boto3 for comprehensive cleanup\n",
    "\n",
    "try:\n",
    "    serverless_predictor.delete_endpoint()\n",
    "    print(\"✅ Serverless endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not delete serverless endpoint: {e}\")\n",
    "\n",
    "try:\n",
    "    churn_model.delete_model()\n",
    "    print(\"Model deleted\")\n",
    "except Exception as e:\n",
    "    print(\"Could not delete churn model: {e}\")\n",
    "\n",
    "sagemaker_client = sagemaker_session.sagemaker_client\n",
    "\n",
    "# List any remaining resources for verification\n",
    "print(\"\\n📋 Checking for remaining resources...\")\n",
    "try:\n",
    "    # Check for any endpoints with our prefix\n",
    "    remaining_endpoints = sagemaker_client.list_endpoints(\n",
    "        NameContains='customer-churn-pytorch',\n",
    "        MaxResults=10\n",
    "    )\n",
    "    if remaining_endpoints['Endpoints']:\n",
    "        print(f\"⚠️  Found {len(remaining_endpoints['Endpoints'])} remaining endpoints\")\n",
    "        for ep in remaining_endpoints['Endpoints']:\n",
    "            print(f\"   - {ep['EndpointName']}\")\n",
    "    else:\n",
    "        print(\"✅ No remaining endpoints found\")\n",
    "        \n",
    "    # Check for endpoint configs\n",
    "    remaining_configs = sagemaker_client.list_endpoint_configs(\n",
    "        NameContains='customer-churn-pytorch',\n",
    "        MaxResults=10\n",
    "    )\n",
    "    if remaining_configs['EndpointConfigs']:\n",
    "        print(f\"⚠️  Found {len(remaining_configs['EndpointConfigs'])} remaining endpoint configs\")\n",
    "        for config in remaining_configs['EndpointConfigs']:\n",
    "            print(f\"   - {config['EndpointConfigName']}\")\n",
    "    else:\n",
    "        print(\"✅ No remaining endpoint configs found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not list remaining resources: {e}\")\n",
    "\n",
    "print(\"\\n✨ Cleanup completed!\")\n",
    "print(f\"   Storage location: {lab_session.base_s3_uri}\")\n",
    "print(\"\\n📝 Remember to delete S3 data when you're completely done!\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: customer-churn-pytorch-serverless-endpoint\n",
      "INFO:sagemaker:Deleting model with name: pytorch-xgboost-churn-2025-10-17-12-37-32-511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Cleaning up resources...\n",
      "⚠️  Could not delete serverless endpoint: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"customer-churn-pytorch-serverless-endpoint\".\n",
      "Could not delete churn model: {e}\n",
      "\n",
      "📋 Checking for remaining resources...\n",
      "✅ No remaining endpoints found\n",
      "✅ No remaining endpoint configs found\n",
      "\n",
      "✨ Cleanup completed!\n",
      "   Storage location: s3://sagemaker-eu-central-1-136548476532/sagemaker_sdk_notebook/2025-10-17T07-48-24\n",
      "\n",
      "📝 Remember to delete S3 data when you're completely done!\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: SageMaker SDK vs sagemaker-core\n",
    "\n",
    "This notebook demonstrates the dramatic improvements in developer experience when using the SageMaker SDK:\n",
    "\n",
    "### Code Reduction:\n",
    "- **Training**: 50+ lines → 10 lines (80% reduction)\n",
    "- **Hyperparameter Tuning**: 40+ lines → 15 lines (70% reduction)  \n",
    "- **Deployment**: 30+ lines → 5 lines (85% reduction)\n",
    "- **Inference**: Fiddly response parsing → Clean `.predict()` calls\n",
    "\n",
    "### Developer Experience:\n",
    "- ✅ **Intuitive**: ML-focused abstractions (Estimators, Predictors)\n",
    "- ✅ **Less error-prone**: Automatic configuration and validation\n",
    "- ✅ **Cleaner inference**: No manual response parsing\n",
    "- ✅ **Better debugging**: Framework-specific error handling\n",
    "- ✅ **Local mode**: Test everything locally before deployment\n",
    "\n",
    "### When to use each:\n",
    "- **SageMaker SDK**: ML development, experimentation, production ML workflows\n",
    "- **sagemaker-core**: Infrastructure management, custom tooling, precise AWS API control\n",
    "\n",
    "### Best of both worlds:\n",
    "Our `CoreLabSession` provides session management while SageMaker SDK handles ML operations - giving you both control and convenience!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
