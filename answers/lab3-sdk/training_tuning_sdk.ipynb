{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SageMaker SDK Training & Hyperparameter Tuning\n\n**Lab 3 - Assignments 2 & 3 Answer Notebook**\n\nThis notebook demonstrates model training and hyperparameter tuning using the SageMaker Python SDK, answering:\n- **Assignment 2**: Training with Framework Estimators\n- **Assignment 3**: Hyperparameter Tuning with the SDK\n\n**Key Benefits of SDK Approach:**\n- 80% less code compared to sagemaker-core\n- High-level ML abstractions (Estimators, Tuners, Predictors)\n- Automatic handling of AWS resource configuration\n- Clean inference with automatic serialization\n- Integrated hyperparameter tuning workflow"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "We'll use our existing `CoreLabSession` for session management but switch to SageMaker SDK for ML operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "# Use our custom session for authentication and S3 management\n",
    "lab_session = CoreLabSession('pytorch', 'customer-churn',\n",
    "                            default_folder='sagemaker_sdk_notebook', \n",
    "                            create_run_folder=True,\n",
    "                             aws_profile='sagemaker-role')\n",
    "lab_session.print()\n",
    "\n",
    "# Get SageMaker session for SDK integration\n",
    "sagemaker_session = lab_session.get_sagemaker_session()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Same data preparation as the core notebook - this part doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = lab_session.core_session.read_s3_file(\n",
    "    f\"sagemaker-example-files-prod-{lab_session.region}\", \n",
    "    \"datasets/tabular/synthetic/churn.txt\"\n",
    ")\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['Churn?'].value_counts(normalize=True)['True.']:.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "df = df.drop(\"Phone\", axis=1)  # Remove unique identifier\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)  # Treat as categorical\n",
    "\n",
    "# Remove highly correlated features (charges are derived from minutes)\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "model_data = pd.get_dummies(df)\n",
    "\n",
    "# Move target to first column (XGBoost convention)\n",
    "model_data = pd.concat([\n",
    "    model_data[\"Churn?_True.\"],\n",
    "    model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Processed data shape: {model_data.shape}\")\n",
    "print(f\"Features: {model_data.shape[1] - 1}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split into train/validation/test\n",
    "train_data, temp_data = train_test_split(model_data, test_size=0.33, random_state=42)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]} samples\")\n",
    "print(f\"Validation: {validation_data.shape[0]} samples\") \n",
    "print(f\"Test: {test_data.shape[0]} samples\")\n",
    "\n",
    "# Save and upload datasets\n",
    "train_data.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "# Store test target separately for evaluation\n",
    "test_target = test_data.iloc[:, 0]  # First column is target\n",
    "test_features = test_data.iloc[:, 1:]  # Rest are features\n",
    "test_features.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3_train_path = lab_session.core_session.upload_data(\"train.csv\")\n",
    "s3_validation_path = lab_session.core_session.upload_data(\"validation.csv\")\n",
    "s3_test_path = lab_session.core_session.upload_data(\"test.csv\")\n",
    "\n",
    "print(f\"\\nData uploaded to S3:\")\n",
    "print(f\"Train: {s3_train_path}\")\n",
    "print(f\"Validation: {s3_validation_path}\")\n",
    "print(f\"Test: {s3_test_path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \ud83c\udf93 Assignment 2: Training with Framework Estimators\n\nThis section demonstrates training with the **PyTorch Framework Estimator** with a custom XGBoost training script - the modern, flexible approach (Lab 3 Option A - Recommended).\n\n**What You'll Learn:**\n- Using PyTorch Framework Estimator for custom training logic\n- Running XGBoost within PyTorch container (modern Python ecosystem)\n- Creating custom training scripts with SageMaker conventions\n- Passing hyperparameters as command-line arguments\n- Automatic dependency installation via requirements.txt\n\n**Key Approach:**\n- **Framework Estimator**: PyTorch (provides modern container environment)\n- **ML Library**: XGBoost (installed via requirements.txt)\n- **Training Script**: Custom `train.py` with full control over training logic\n\n**SDK vs. sagemaker-core:**\nWhat took 50+ lines of TrainingJob configuration becomes just a few lines with the Framework Estimator pattern, while maintaining full flexibility through custom training scripts."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sagemaker.pytorch import PyTorch\n\n# Create PyTorch Framework Estimator with custom XGBoost training script\n# This uses PyTorch container for modern Python ecosystem while training with XGBoost\nxgb_estimator = PyTorch(\n    entry_point='train.py',           # Custom training script\n    source_dir='src/',                # Directory with train.py and requirements.txt\n    framework_version='2.0',          # PyTorch version (not XGBoost!)\n    py_version='py310',\n    role=lab_session.role,\n    instance_type='ml.m5.xlarge',\n    instance_count=1,\n    output_path=lab_session.base_s3_uri,\n    sagemaker_session=sagemaker_session,\n    base_job_name='pytorch-xgboost-churn',\n    \n    # XGBoost hyperparameters (passed to train.py as CLI arguments)\n    # Note: Use hyphens not underscores for CLI arg compatibility\n    hyperparameters={\n        'max-depth': 5,\n        'eta': 0.2,\n        'gamma': 4,\n        'min-child-weight': 6,\n        'subsample': 0.8,\n        'objective': 'binary:logistic',\n        'num-round': 100,\n        'eval-metric': 'auc'\n    }\n)\n\nprint(\"\u2705 PyTorch Framework Estimator configured\")\nprint(f\"Training will use: {xgb_estimator.instance_type}\")\nprint(f\"Entry point: {xgb_estimator.entry_point}\")\nprint(f\"Output location: {xgb_estimator.output_path}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Train the model - just one line!\n",
    "# Compare this to the complex TrainingJob.create() in sagemaker-core\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(f\"\u2705 Training completed!\")\n",
    "print(f\"Model artifacts: {xgb_estimator.model_data}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## \u26a1 Assignment 3: Hyperparameter Tuning with the SDK\n\nThis section demonstrates automated hyperparameter optimization using the **SageMaker SDK's HyperparameterTuner**.\n\n**What You'll Learn:**\n- Defining hyperparameter search spaces with typed parameters\n- Configuring Bayesian optimization strategy\n- Running parallel tuning jobs with resource management\n- Analyzing tuning results and selecting best models\n\n**SDK vs. sagemaker-core:**\nThe HyperparameterTuner class makes tuning much more intuitive compared to the complex HyperParameterTuningJobConfig shapes from Lab 1."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sagemaker.tuner import (\n    HyperparameterTuner,\n    IntegerParameter,\n    ContinuousParameter\n)\n\n# Define hyperparameter ranges - much cleaner than sagemaker-core!\n# Note: Use hyphens to match CLI argument format in train.py\nhyperparameter_ranges = {\n    'max-depth': IntegerParameter(3, 10),\n    'eta': ContinuousParameter(0.01, 0.3),\n    'gamma': ContinuousParameter(0, 5),\n    'min-child-weight': ContinuousParameter(1, 10),\n    'subsample': ContinuousParameter(0.5, 1.0),\n    'num-round': IntegerParameter(50, 200)\n}\n\n# Create tuner with metric definitions\n# IMPORTANT: metric_definitions is required for framework estimators (not built-in algorithms)\n# since SageMaker doesn't know how to parse metrics from custom training scripts\ntuner = HyperparameterTuner(\n    xgb_estimator,\n    objective_metric_name='validation:auc',\n    hyperparameter_ranges=hyperparameter_ranges,\n    metric_definitions=[\n        {'Name': 'validation:auc', 'Regex': 'validation-auc:([0-9\\\\.]+)'},\n        {'Name': 'train:auc', 'Regex': 'train-auc:([0-9\\\\.]+)'}\n    ],\n    max_jobs=10,\n    max_parallel_jobs=3,\n    base_tuning_job_name='pytorch-xgboost-tuning'\n)\n\nprint(\"\u2705 Hyperparameter tuner configured\")\nprint(f\"Will run {tuner.max_jobs} tuning jobs\")\nprint(f\"Optimizing: {tuner.objective_metric_name}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Start tuning - one line vs complex sagemaker-core setup!\ntuner.fit({\n    'train': s3_train_path,\n    'validation': s3_validation_path\n})\n\nprint(\"\u2705 Hyperparameter tuning completed!\")\n\n# Get best training job details using HyperparameterTuningJobAnalytics\n# Note: best_training_job() returns a string (job name), not a dictionary\nfrom sagemaker.analytics import HyperparameterTuningJobAnalytics\n\ntuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name)\nfull_df = tuner_analytics.dataframe()\n\n# Get best training job (highest validation:auc)\nbest_job_row = full_df.sort_values('FinalObjectiveValue', ascending=False).iloc[0]\n\nprint(f\"\\nBest job: {best_job_row['TrainingJobName']}\")\nprint(f\"Best AUC: {best_job_row['FinalObjectiveValue']:.4f}\")\n\nprint(\"\\nBest hyperparameters:\")\nfor key in hyperparameter_ranges.keys():\n    print(f\"  {key}: {best_job_row[key]}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Model Deployment\n\nNow we'll deploy the model using different strategies: provisioned endpoints, serverless endpoints, and batch transform."
  },
  {
   "cell_type": "markdown",
   "source": "## Create PyTorchModel with Custom Inference Handler\n\nBefore deploying, we create a `PyTorchModel` with our custom `inference.py` handler. This model will be reused for all deployment types (provisioned endpoint, serverless endpoint, and batch transform), ensuring consistent inference behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sagemaker.pytorch import PyTorchModel\n\n# Create PyTorchModel with custom inference handler\n# This will be reused for all deployments (endpoints and batch transform)\nbest_estimator = tuner.best_estimator()\n\npytorch_model = PyTorchModel(\n    model_data=best_estimator.model_data,\n    role=lab_session.role,\n    framework_version='2.0',\n    py_version='py310',\n    source_dir='src/',\n    entry_point='inference.py',\n    sagemaker_session=sagemaker_session\n)\n\nprint(\"\u2705 PyTorchModel created with custom inference handler\")\nprint(f\"Model data: {pytorch_model.model_data}\")\nprint(f\"Entry point: {pytorch_model.entry_point}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Deploy the best model from tuning to a provisioned endpoint\n# Uses the PyTorchModel with custom inference.py handler\n\npredictor = pytorch_model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge',\n    endpoint_name=lab_session.endpoint_name\n)\n\nprint(f\"\u2705 Model deployed to endpoint: {predictor.endpoint_name}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from sagemaker.serverless import ServerlessInferenceConfig\n\n# Deploy serverless endpoint using the same PyTorchModel\nserverless_config = ServerlessInferenceConfig(\n    memory_size_in_mb=2048,\n    max_concurrency=10,\n)\n\nserverless_predictor = pytorch_model.deploy(\n    serverless_inference_config=serverless_config,\n    endpoint_name=lab_session.serverless_endpoint_name\n)\n\nprint(f\"\u2705 Serverless model deployed: {serverless_predictor.endpoint_name}\")\nprint(f\"Memory: {serverless_config.memory_size_in_mb}MB\")\nprint(f\"Max concurrency: {serverless_config.max_concurrency}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:31:08.493986Z",
     "start_time": "2025-10-10T12:26:59.227225Z"
    }
   },
   "source": "## Batch Transform\n\nThe SageMaker SDK also simplifies batch inference.\n\n**Key Points:**\n\n1. **Reusing PyTorchModel**: We use the same `PyTorchModel` created earlier that includes our custom `inference.py` handler. This ensures consistent inference behavior across endpoints and batch transform.\n\n2. **Custom Inference Handler**: The `inference.py` script handles XGBoost models in the PyTorch container with four functions:\n   - `model_fn()`: Load the XGBoost model from disk\n   - `input_fn()`: Parse CSV input into XGBoost DMatrix (handles structured arrays)\n   - `predict_fn()`: Run inference with the model\n   - `output_fn()`: Format predictions as CSV output"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Create transformer from the PyTorchModel with custom inference handler\n# Uses the same pytorch_model we created earlier with inference.py\n\ntransformer = pytorch_model.transformer(\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    output_path=lab_session.transform_output_s3_uri,\n)\n\n# Run batch transform\ntransformer.transform(\n    data=s3_test_path,\n    content_type='text/csv',\n    split_type='Line'\n)\n\nprint(f\"\u2705 Batch transform completed!\")\nprint(f\"Results saved to: {transformer.output_path}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Inference\n",
    "\n",
    "This is where the SageMaker SDK really shines - compare this clean interface to the fiddly `invoke()` + `read()` + `decode()` + `split()` in sagemaker-core!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Test both endpoints with clean interface - no more fiddly response parsing!\n",
    "sample_data = test_features.head(10).values\n",
    "\n",
    "print(\"=== ENDPOINT COMPARISON ===\")\n",
    "print(f\"Testing with {len(sample_data)} samples\\n\")\n",
    "\n",
    "# Provisioned endpoint\n",
    "print(\"\ud83d\udda5\ufe0f  PROVISIONED ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "provisioned_predictions = predictor.predict(sample_data)  # Clean and simple!\n",
    "provisioned_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(provisioned_predictions).shape}\")\n",
    "print(f\"   Latency: {provisioned_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {provisioned_predictions[:3]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Serverless endpoint  \n",
    "print(\"\u2601\ufe0f  SERVERLESS ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "serverless_predictions = serverless_predictor.predict(sample_data)  # Also clean!\n",
    "serverless_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(serverless_predictions).shape}\")\n",
    "print(f\"   Latency: {serverless_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {serverless_predictions[:3]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compare results\n",
    "predictions_match = np.allclose(provisioned_predictions, serverless_predictions, rtol=1e-5)\n",
    "print(f\"\u2705 Predictions match: {predictions_match}\")\n",
    "print(f\"\ud83d\udcca Latency difference: {abs(serverless_latency - provisioned_latency):.1f}ms\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate on full test set\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "\n",
    "# Get predictions for full test set\n",
    "test_predictions = predictor.predict(test_features.values)\n",
    "test_probabilities = np.array(test_predictions)\n",
    "test_binary = (test_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_target, test_binary)\n",
    "precision = precision_score(test_target, test_binary)\n",
    "recall = recall_score(test_target, test_binary)\n",
    "auc = roc_auc_score(test_target, test_probabilities)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  ROC AUC:   {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcca Tested on {len(test_target)} samples\")\n",
    "print(f\"\ud83c\udfaf Churn rate in test set: {test_target.mean():.1%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The SageMaker SDK also makes cleanup simpler with built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean up resources - much simpler than sagemaker-core!\n",
    "\n",
    "print(\"\ud83e\uddf9 Cleaning up resources...\")\n",
    "\n",
    "# Delete endpoints (predictors handle the cleanup automatically)\n",
    "print(\"\\n\ud83d\uddd1\ufe0f  Deleting endpoints...\")\n",
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    print(\"\u2705 Provisioned endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error deleting provisioned endpoint: {e}\")\n",
    "\n",
    "try:\n",
    "    serverless_predictor.delete_endpoint()\n",
    "    print(\"\u2705 Serverless endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error deleting serverless endpoint: {e}\")\n",
    "\n",
    "# Note: The SageMaker SDK automatically cleans up endpoint configs\n",
    "# and models when deleting endpoints (unless they're shared)\n",
    "\n",
    "print(\"\\n\u2728 Cleanup completed!\")\n",
    "print(\"\\n\ud83d\udcb0 Cost Summary:\")\n",
    "print(f\"   Training time: ~2-3 minutes\")\n",
    "print(f\"   Tuning time: ~10-15 minutes\")\n",
    "print(f\"   Inference time: ~5 minutes\")\n",
    "print(f\"   Storage location: {lab_session.base_s3_uri}\")\n",
    "print(\"\\n\ud83d\udcdd Remember to delete S3 data when you're completely done!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: SageMaker SDK vs sagemaker-core\n",
    "\n",
    "This notebook demonstrates the dramatic improvements in developer experience when using the SageMaker SDK:\n",
    "\n",
    "### Code Reduction:\n",
    "- **Training**: 50+ lines \u2192 10 lines (80% reduction)\n",
    "- **Hyperparameter Tuning**: 40+ lines \u2192 15 lines (70% reduction)  \n",
    "- **Deployment**: 30+ lines \u2192 5 lines (85% reduction)\n",
    "- **Inference**: Fiddly response parsing \u2192 Clean `.predict()` calls\n",
    "\n",
    "### Developer Experience:\n",
    "- \u2705 **Intuitive**: ML-focused abstractions (Estimators, Predictors)\n",
    "- \u2705 **Less error-prone**: Automatic configuration and validation\n",
    "- \u2705 **Cleaner inference**: No manual response parsing\n",
    "- \u2705 **Better debugging**: Framework-specific error handling\n",
    "- \u2705 **Local mode**: Test everything locally before deployment\n",
    "\n",
    "### When to use each:\n",
    "- **SageMaker SDK**: ML development, experimentation, production ML workflows\n",
    "- **sagemaker-core**: Infrastructure management, custom tooling, precise AWS API control\n",
    "\n",
    "### Best of both worlds:\n",
    "Our `CoreLabSession` provides session management while SageMaker SDK handles ML operations - giving you both control and convenience!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}