{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker SDK Training & Hyperparameter Tuning\n",
    "\n",
    "**Lab 3 - Assignments 2 & 3 Answer Notebook**\n",
    "\n",
    "This notebook demonstrates model training and hyperparameter tuning using the SageMaker Python SDK, answering:\n",
    "- **Assignment 2**: Training with Framework Estimators\n",
    "- **Assignment 3**: Hyperparameter Tuning with the SDK\n",
    "\n",
    "**Key Benefits of SDK Approach:**\n",
    "- 80% less code compared to sagemaker-core\n",
    "- High-level ML abstractions (Estimators, Tuners, Predictors)\n",
    "- Automatic handling of AWS resource configuration\n",
    "- Clean inference with automatic serialization\n",
    "- Integrated hyperparameter tuning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "We'll use our existing `CoreLabSession` for session management but switch to SageMaker SDK for ML operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:05:15.195726Z",
     "start_time": "2025-10-12T18:05:12.717648Z"
    }
   },
   "outputs": [],
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "# Use our custom session for authentication and S3 management\n",
    "lab_session = CoreLabSession('pytorch', 'customer-churn',\n",
    "                            default_folder='sagemaker_sdk_notebook', \n",
    "                            create_run_folder=True,\n",
    "                             aws_profile='sagemaker-role')\n",
    "lab_session.print()\n",
    "\n",
    "# Get SageMaker session for SDK integration\n",
    "sagemaker_session = lab_session.get_sagemaker_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Same data preparation as the core notebook - this part doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:05:18.971288Z",
     "start_time": "2025-10-12T18:05:18.641153Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = lab_session.core_session.read_s3_file(\n",
    "    f\"sagemaker-example-files-prod-{lab_session.region}\", \n",
    "    \"datasets/tabular/synthetic/churn.txt\"\n",
    ")\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['Churn?'].value_counts(normalize=True)['True.']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:05:20.489707Z",
     "start_time": "2025-10-12T18:05:20.435664Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df = df.drop(\"Phone\", axis=1)  # Remove unique identifier\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)  # Treat as categorical\n",
    "\n",
    "# Remove highly correlated features (charges are derived from minutes)\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "model_data = pd.get_dummies(df)\n",
    "\n",
    "# Move target to first column (XGBoost convention)\n",
    "model_data = pd.concat([\n",
    "    model_data[\"Churn?_True.\"],\n",
    "    model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Processed data shape: {model_data.shape}\")\n",
    "print(f\"Features: {model_data.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T18:05:22.527864Z",
     "start_time": "2025-10-12T18:05:21.693745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into train/validation/test\n",
    "train_data, temp_data = train_test_split(model_data, test_size=0.33, random_state=42)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]} samples\")\n",
    "print(f\"Validation: {validation_data.shape[0]} samples\") \n",
    "print(f\"Test: {test_data.shape[0]} samples\")\n",
    "\n",
    "# Save and upload datasets\n",
    "train_data.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "# Store test target separately for evaluation\n",
    "test_target = test_data.iloc[:, 0]  # First column is target\n",
    "test_features = test_data.iloc[:, 1:]  # Rest are features\n",
    "test_features.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3_train_path = lab_session.core_session.upload_data(\"train.csv\")\n",
    "s3_validation_path = lab_session.core_session.upload_data(\"validation.csv\")\n",
    "s3_test_path = lab_session.core_session.upload_data(\"test.csv\")\n",
    "\n",
    "print(f\"\\nData uploaded to S3:\")\n",
    "print(f\"Train: {s3_train_path}\")\n",
    "print(f\"Validation: {s3_validation_path}\")\n",
    "print(f\"Test: {s3_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Assignment 2: Training with Framework Estimators\n",
    "\n",
    "This section demonstrates training with the **PyTorch Framework Estimator** with a custom XGBoost training script - the modern, flexible approach (Lab 3 Option A - Recommended).\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Using PyTorch Framework Estimator for custom training logic\n",
    "- Running XGBoost within PyTorch container (modern Python ecosystem)\n",
    "- Creating custom training scripts with SageMaker conventions\n",
    "- Passing hyperparameters as command-line arguments\n",
    "- Automatic dependency installation via requirements.txt\n",
    "\n",
    "**Key Approach:**\n",
    "- **Framework Estimator**: PyTorch (provides modern container environment)\n",
    "- **ML Library**: XGBoost (installed via requirements.txt)\n",
    "- **Training Script**: Custom `train.py` with full control over training logic\n",
    "\n",
    "**SDK vs. sagemaker-core:**\n",
    "What took 50+ lines of TrainingJob configuration becomes just a few lines with the Framework Estimator pattern, while maintaining full flexibility through custom training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:20:00.437692Z",
     "start_time": "2025-10-12T19:20:00.380769Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# Create PyTorch Framework Estimator with custom XGBoost training script\n",
    "# This uses PyTorch container for modern Python ecosystem while training with XGBoost\n",
    "xgb_estimator = PyTorch(\n",
    "    entry_point='train.py',           # Custom training script\n",
    "    source_dir='src/',                # Directory with train.py and requirements.txt\n",
    "    framework_version='2.6.0',          # PyTorch version (not XGBoost!)\n",
    "    py_version='py312',\n",
    "    role=lab_session.role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    output_path=lab_session.base_s3_uri,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='pytorch-xgboost-churn',\n",
    "\n",
    "    # XGBoost hyperparameters (passed to train.py as CLI arguments)\n",
    "    # Note: Use hyphens not underscores for CLI arg compatibility\n",
    "    hyperparameters={\n",
    "        'max-depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min-child-weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num-round': 100,\n",
    "        'eval-metric': 'auc'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ PyTorch Framework Estimator configured\")\n",
    "print(f\"Training will use: {xgb_estimator.instance_type}\")\n",
    "print(f\"Entry point: {xgb_estimator.entry_point}\")\n",
    "print(f\"Output location: {xgb_estimator.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:27:10.110758Z",
     "start_time": "2025-10-12T19:23:49.947002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train the model - just one line!\n",
    "# Compare this to the complex TrainingJob.create() in sagemaker-core\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(f\"✅ Training completed!\")\n",
    "print(f\"Model artifacts: {xgb_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Assignment 3: Hyperparameter Tuning with the SDK\n",
    "\n",
    "This section demonstrates automated hyperparameter optimization using the **SageMaker SDK's HyperparameterTuner**.\n",
    "\n",
    "**What You'll Learn:**\n",
    "- Defining hyperparameter search spaces with typed parameters\n",
    "- Configuring Bayesian optimization strategy\n",
    "- Running parallel tuning jobs with resource management\n",
    "- Analyzing tuning results and selecting best models\n",
    "\n",
    "**SDK vs. sagemaker-core:**\n",
    "The HyperparameterTuner class makes tuning much more intuitive compared to the complex HyperParameterTuningJobConfig shapes from Lab 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner,\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges - much cleaner than sagemaker-core!\n",
    "# Note: Use hyphens to match CLI argument format in train.py\n",
    "hyperparameter_ranges = {\n",
    "    'max-depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min-child-weight': ContinuousParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'num-round': IntegerParameter(50, 200)\n",
    "}\n",
    "\n",
    "# Create tuner with metric definitions\n",
    "# IMPORTANT: metric_definitions is required for framework estimators (not built-in algorithms)\n",
    "# since SageMaker doesn't know how to parse metrics from custom training scripts\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=[\n",
    "        {'Name': 'validation:auc', 'Regex': 'validation-auc:([0-9\\\\.]+)'},\n",
    "        {'Name': 'train:auc', 'Regex': 'train-auc:([0-9\\\\.]+)'}\n",
    "    ],\n",
    "    max_jobs=3,\n",
    "    max_parallel_jobs=3,\n",
    "    base_tuning_job_name='pytorch-xgboost-tuning'\n",
    ")\n",
    "\n",
    "print(\"✅ Hyperparameter tuner configured\")\n",
    "print(f\"Will run {tuner.max_jobs} tuning jobs\")\n",
    "print(f\"Optimizing: {tuner.objective_metric_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tuning - one line vs complex sagemaker-core setup!\n",
    "tuner.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(\"✅ Hyperparameter tuning completed!\")\n",
    "\n",
    "# Get best training job details using HyperparameterTuningJobAnalytics\n",
    "# Note: best_training_job() returns a string (job name), not a dictionary\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "tuner_analytics = HyperparameterTuningJobAnalytics(tuner.latest_tuning_job.name)\n",
    "full_df = tuner_analytics.dataframe()\n",
    "\n",
    "# Get best training job (highest validation:auc)\n",
    "best_job_row = full_df.sort_values('FinalObjectiveValue', ascending=False).iloc[0]\n",
    "\n",
    "print(f\"\\nBest job: {best_job_row['TrainingJobName']}\")\n",
    "print(f\"Best AUC: {best_job_row['FinalObjectiveValue']:.4f}\")\n",
    "\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key in hyperparameter_ranges.keys():\n",
    "    print(f\"  {key}: {best_job_row[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "Now we'll deploy the model using different strategies: provisioned endpoints, serverless endpoints, and batch transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PyTorchModel with Custom Inference Handler\n",
    "\n",
    "Before deploying, we create a `PyTorchModel` with our custom `inference.py` handler. This model will be reused for all deployment types (provisioned endpoint, serverless endpoint, and batch transform), ensuring consistent inference behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:53:49.606289Z",
     "start_time": "2025-10-12T19:53:43.868878Z"
    }
   },
   "outputs": [],
   "source": [
    "final_estimator = tuner.best_estimator() if 'tuner' in locals() else xgb_estimator\n",
    "\n",
    "# Create PyTorchModel with custom inference handler\n",
    "# This will be reused for all deployments (endpoints and batch transform)\n",
    "\n",
    "pytorch_model = xgb_estimator.create_model(source_dir=xgb_estimator.source_dir, entry_point='inference.py')\n",
    "\n",
    "print(\"✅ PyTorchModel created with custom inference handler\")\n",
    "print(f\"Model data: {pytorch_model.model_data}\")\n",
    "print(f\"Entry point: {pytorch_model.entry_point}\")\n",
    "print(f\"Model name: {pytorch_model.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:34:01.152718Z",
     "start_time": "2025-10-12T19:34:01.108370Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:56:25.523805Z",
     "start_time": "2025-10-12T19:53:53.073876Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Deploy serverless endpoint using the same PyTorchModel\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,\n",
    "    max_concurrency=10,\n",
    ")\n",
    "\n",
    "serverless_predictor = pytorch_model.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=lab_session.serverless_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"✅ Serverless model deployed: {serverless_predictor.endpoint_name}\")\n",
    "print(f\"Memory: {serverless_config.memory_size_in_mb}MB\")\n",
    "print(f\"Max concurrency: {serverless_config.max_concurrency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-10T12:31:08.493986Z",
     "start_time": "2025-10-10T12:26:59.227225Z"
    }
   },
   "source": [
    "## Batch Transform\n",
    "\n",
    "The SageMaker SDK also simplifies batch inference.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Reusing PyTorchModel**: We use the same `PyTorchModel` created earlier that includes our custom `inference.py` handler. This ensures consistent inference behavior across endpoints and batch transform.\n",
    "\n",
    "2. **Custom Inference Handler**: The `inference.py` script handles XGBoost models in the PyTorch container with four functions:\n",
    "   - `model_fn()`: Load the XGBoost model from disk\n",
    "   - `input_fn()`: Parse CSV input into XGBoost DMatrix (handles structured arrays)\n",
    "   - `predict_fn()`: Run inference with the model\n",
    "   - `output_fn()`: Format predictions as CSV output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer from the PyTorchModel with custom inference handler\n",
    "# Uses the same pytorch_model we created earlier with inference.py\n",
    "\n",
    "transformer = pytorch_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=lab_session.transform_output_s3_uri,\n",
    ")\n",
    "\n",
    "# Run batch transform\n",
    "transformer.transform(\n",
    "    data=s3_test_path,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "print(f\"✅ Batch transform completed!\")\n",
    "print(f\"Results saved to: {transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Inference\n",
    "\n",
    "This is where the SageMaker SDK really shines - compare this clean interface to the fiddly `invoke()` + `read()` + `decode()` + `split()` in sagemaker-core!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T20:07:07.064765Z",
     "start_time": "2025-10-12T20:07:06.912766Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Test both endpoints with clean interface - no more fiddly response parsing!\n",
    "sample_data = test_features.head(10).values\n",
    "\n",
    "# Serverless endpoint  \n",
    "print(\"☁️  SERVERLESS ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "serverless_predictions = serverless_predictor.predict(sample_data)  # Also clean!\n",
    "serverless_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(serverless_predictions).shape}\")\n",
    "print(f\"   Latency: {serverless_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {serverless_predictions[:4]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compare results\n",
    "# predictions_match = np.allclose(provisioned_predictions, serverless_predictions, rtol=1e-5)\n",
    "# print(f\"✅ Predictions match: {predictions_match}\")\n",
    "# print(f\"📊 Latency difference: {abs(serverless_latency - provisioned_latency):.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T20:07:48.460683Z",
     "start_time": "2025-10-12T20:07:48.197827Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on full test set\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "\n",
    "# Get predictions for full test set\n",
    "test_predictions = serverless_predictor.predict(test_features.values)\n",
    "test_probabilities = np.array(test_predictions)\n",
    "test_binary = (test_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_target, test_binary)\n",
    "precision = precision_score(test_target, test_binary)\n",
    "recall = recall_score(test_target, test_binary)\n",
    "auc = roc_auc_score(test_target, test_probabilities)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  ROC AUC:   {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Tested on {len(test_target)} samples\")\n",
    "print(f\"🎯 Churn rate in test set: {test_target.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The SageMaker SDK also makes cleanup simpler with built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T19:53:18.673095Z",
     "start_time": "2025-10-12T19:53:16.736521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up resources - comprehensive cleanup including configurations and models\n",
    "\n",
    "print(\"🧹 Cleaning up resources...\")\n",
    "\n",
    "# Import boto3 for comprehensive cleanup\n",
    "import boto3\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=lab_session.region)\n",
    "\n",
    "try:\n",
    "    serverless_predictor.delete_endpoint(delete_endpoint_config=False)  # Don't auto-delete config  \n",
    "    print(\"✅ Serverless endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not delete serverless endpoint: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    serverless_config_name = serverless_predictor.endpoint_name\n",
    "    sagemaker_client.describe_endpoint_config(EndpointConfigName=serverless_config_name)\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=serverless_config_name)\n",
    "    print(f\"✅ Deleted endpoint config: {serverless_config_name}\")\n",
    "except sagemaker_client.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ValidationException':\n",
    "        print(f\"ℹ️  Endpoint config {serverless_config_name} not found or already deleted\")\n",
    "    else:\n",
    "        print(f\"⚠️  Could not delete serverless endpoint config: {e}\")\n",
    "\n",
    "# Delete models\n",
    "print(\"\\n🗑️  Deleting models...\")\n",
    "try:\n",
    "    # For the serverless endpoint\n",
    "    try:\n",
    "        response = sagemaker_client.describe_endpoint(EndpointName=serverless_predictor.endpoint_name)\n",
    "        config_name = response['EndpointConfigName']\n",
    "        config_response = sagemaker_client.describe_endpoint_config(EndpointConfigName=config_name)\n",
    "        model_name = config_response['ProductionVariants'][0]['ModelName']\n",
    "        sagemaker_client.delete_model(ModelName=model_name)\n",
    "        print(f\"✅ Deleted model: {model_name}\")\n",
    "    except:\n",
    "        pass  # Model might already be deleted\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ℹ️  Some models may not have been deleted (they might be shared or already deleted)\")\n",
    "\n",
    "# List any remaining resources for verification\n",
    "print(\"\\n📋 Checking for remaining resources...\")\n",
    "try:\n",
    "    # Check for any endpoints with our prefix\n",
    "    remaining_endpoints = sagemaker_client.list_endpoints(\n",
    "        NameContains='customer-churn-pytorch',\n",
    "        MaxResults=10\n",
    "    )\n",
    "    if remaining_endpoints['Endpoints']:\n",
    "        print(f\"⚠️  Found {len(remaining_endpoints['Endpoints'])} remaining endpoints\")\n",
    "        for ep in remaining_endpoints['Endpoints']:\n",
    "            print(f\"   - {ep['EndpointName']}\")\n",
    "    else:\n",
    "        print(\"✅ No remaining endpoints found\")\n",
    "        \n",
    "    # Check for endpoint configs\n",
    "    remaining_configs = sagemaker_client.list_endpoint_configs(\n",
    "        NameContains='customer-churn-pytorch',\n",
    "        MaxResults=10\n",
    "    )\n",
    "    if remaining_configs['EndpointConfigs']:\n",
    "        print(f\"⚠️  Found {len(remaining_configs['EndpointConfigs'])} remaining endpoint configs\")\n",
    "        for config in remaining_configs['EndpointConfigs']:\n",
    "            print(f\"   - {config['EndpointConfigName']}\")\n",
    "    else:\n",
    "        print(\"✅ No remaining endpoint configs found\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not list remaining resources: {e}\")\n",
    "\n",
    "print(\"\\n✨ Cleanup completed!\")\n",
    "print(f\"   Storage location: {lab_session.base_s3_uri}\")\n",
    "print(\"\\n📝 Remember to delete S3 data when you're completely done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: SageMaker SDK vs sagemaker-core\n",
    "\n",
    "This notebook demonstrates the dramatic improvements in developer experience when using the SageMaker SDK:\n",
    "\n",
    "### Code Reduction:\n",
    "- **Training**: 50+ lines → 10 lines (80% reduction)\n",
    "- **Hyperparameter Tuning**: 40+ lines → 15 lines (70% reduction)  \n",
    "- **Deployment**: 30+ lines → 5 lines (85% reduction)\n",
    "- **Inference**: Fiddly response parsing → Clean `.predict()` calls\n",
    "\n",
    "### Developer Experience:\n",
    "- ✅ **Intuitive**: ML-focused abstractions (Estimators, Predictors)\n",
    "- ✅ **Less error-prone**: Automatic configuration and validation\n",
    "- ✅ **Cleaner inference**: No manual response parsing\n",
    "- ✅ **Better debugging**: Framework-specific error handling\n",
    "- ✅ **Local mode**: Test everything locally before deployment\n",
    "\n",
    "### When to use each:\n",
    "- **SageMaker SDK**: ML development, experimentation, production ML workflows\n",
    "- **sagemaker-core**: Infrastructure management, custom tooling, precise AWS API control\n",
    "\n",
    "### Best of both worlds:\n",
    "Our `CoreLabSession` provides session management while SageMaker SDK handles ML operations - giving you both control and convenience!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
