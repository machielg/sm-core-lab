{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# SageMaker Core Pipeline - Data Prep, Training, and Model Creation\n",
    "\n",
    "This notebook demonstrates how to create a complete ML pipeline using SageMaker Core that includes:\n",
    "1. Data Processing - Prepare and split the customer churn dataset\n",
    "2. Model Training - Train an XGBoost model on processed data\n",
    "3. Model Evaluation - Evaluate the trained model on holdout data\n",
    "4. Model Creation - Create a deployable SageMaker model from training artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initialize CoreLab Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "lab_session = CoreLabSession(\n",
    "    'xgboost',\n",
    "    'customer-churn-pipeline',\n",
    "    default_folder='pipeline_notebook',\n",
    "    create_run_folder=True,\n",
    "    aws_profile='sagemaker-role'\n",
    ")\n",
    "lab_session.print()\n",
    "core_session = lab_session.core_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Import SageMaker Pipeline Components\n",
    "\n",
    "Note: SageMaker Pipelines SDK (not sagemaker-core) is used for pipeline orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline-specific imports from SageMaker SDK\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterFloat,\n",
    "    ParameterString\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Processing imports - using XGBoostProcessor for better framework integration\n",
    "from sagemaker.xgboost.processing import XGBoostProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "# Training imports  \n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Model imports\n",
    "from sagemaker.model import Model\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All pipeline modules imported successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PipelineSession for proper pipeline execution context\n",
    "pipeline_session = PipelineSession(\n",
    "    boto_session=lab_session.core_session.boto_session,\n",
    "    default_bucket=lab_session.core_session.default_bucket(),\n",
    "    default_bucket_prefix=lab_session.core_session.default_bucket_prefix\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“¦ Default bucket: {pipeline_session.default_bucket()}\")\n",
    "print(f\"ðŸ“ Bucket prefix: {pipeline_session.default_bucket_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Define input and output locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define data locations\n",
    "data_s3_uri = f\"s3://sagemaker-example-files-prod-{lab_session.region}/datasets/tabular/synthetic/churn.txt\"\n",
    "pipeline_output_s3_uri = lab_session.jobs_output_s3_uri\n",
    "\n",
    "print(f\"ðŸ“ Data S3 URI: {data_s3_uri}\")\n",
    "print(f\"ðŸ“¤ Pipeline Output S3 URI: {pipeline_output_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Define Pipeline Parameters\n",
    "\n",
    "Pipeline parameters allow us to customize pipeline executions without modifying the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pipeline parameters for flexibility\n",
    "\n",
    "# Processing parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "train_test_split = ParameterFloat(\n",
    "    name=\"TrainTestSplit\",\n",
    "    default_value=0.33\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "max_depth = ParameterString(\n",
    "    name=\"MaxDepth\",\n",
    "    default_value=\"5\"\n",
    ")\n",
    "\n",
    "num_round = ParameterString(\n",
    "    name=\"NumRound\",\n",
    "    default_value=\"100\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Step 1: Define Processing Step\n",
    "\n",
    "This step processes raw data and splits it into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "# Create XGBoostProcessor - framework-aware with better dependency handling\n",
    "xgb_processor = XGBoostProcessor(\n",
    "    framework_version='1.7-1',\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    role=lab_session.role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    volume_size_in_gb=30,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    env={\"PYTHONUNBUFFERED\": \"1\"},\n",
    "    base_job_name='churn-preprocessing'\n",
    ")\n",
    "\n",
    "src_dir = Path(os.getcwd(), '..', 'processing', 'src').resolve()\n",
    "print(src_dir, src_dir.exists())\n",
    "# Use step_args pattern for proper pipeline integration\n",
    "processor_args = xgb_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    source_dir=str(src_dir) + '/',  # Directory with code and requirements.txt\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=data_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/data\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            # destination=f\"{pipeline_output_s3_uri}/data/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\", \n",
    "            source=\"/opt/ml/processing/output/validation\",\n",
    "            # destination=f\"{pipeline_output_s3_uri}/data/validation\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            # destination=f\"{pipeline_output_s3_uri}/data/test\"\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\"--train-test-split\", train_test_split.to_string()]\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessCustomerChurnData\",\n",
    "    step_args=processor_args\n",
    ")\n",
    "\n",
    "print(\"âœ… Processing step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Step 2: Define Training Step\n",
    "\n",
    "This step trains an XGBoost model using the processed data from Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost estimator using generic Estimator with XGBoost image, train.py not needed\n",
    "xgboost_estimator = Estimator(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    role=lab_session.role,\n",
    "    output_path=f\"{pipeline_output_s3_uri}/models\",\n",
    "    sagemaker_session=pipeline_session,\n",
    "    hyperparameters={\n",
    "        \"max_depth\": max_depth,\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.8\",\n",
    "        \"verbosity\": \"0\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": num_round\n",
    "    }\n",
    ")\n",
    "\n",
    "# Use step_args pattern for training step\n",
    "training_args = xgboost_estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainXGBoostModel\",\n",
    "    step_args=training_args\n",
    ")\n",
    "\n",
    "print(\"âœ… Training step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Step 3: Define Evaluation Step\n",
    "\n",
    "This step evaluates the trained model against the validation dataset created during preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ScriptProcessor for evaluation to run custom metrics\n",
    "evaluation_src_dir = Path(os.getcwd(), 'src').resolve()\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    command=['python3'],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    role=lab_session.role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    base_job_name='churn-evaluation',\n",
    "    volume_size_in_gb=30,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    env={'PYTHONUNBUFFERED': '1'}\n",
    ")\n",
    "\n",
    "evaluation_args = script_processor.run(\n",
    "    code='evaluate.py',\n",
    "    source_dir=str(evaluation_src_dir) + '/',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination='/opt/ml/processing/model'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri,\n",
    "            destination='/opt/ml/processing/evaluation'\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/output'\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json'\n",
    ")\n",
    "\n",
    "step_evaluate = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    step_args=evaluation_args,\n",
    "    property_files=[evaluation_report]\n",
    ")\n",
    "\n",
    "print('âœ… Evaluation step defined')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Step 4: Define Model Creation Step\n",
    "\n",
    "This step creates a SageMaker Model from the trained model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Model object using pipeline session for consistency\n",
    "model = Model(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=lab_session.role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "# Use step_args pattern for model creation\n",
    "model_create_args = model.create(instance_type=\"ml.m5.large\")\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateXGBoostModel\",\n",
    "    step_args=model_create_args\n",
    ")\n",
    "\n",
    "print(\"âœ… Model creation step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Optional: Model Registry Step\n",
    "\n",
    "Register the model in SageMaker Model Registry for versioning and deployment management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "  model_statistics=MetricsSource(\n",
    "      s3_uri=Join(on=\"/\", values=[\n",
    "          step_evaluate.properties.ProcessingOutputConfig.Outputs[\"evaluation\"].S3Output.S3Uri,\n",
    "          \"evaluation.json\"\n",
    "      ]),\n",
    "      content_type=\"application/json\"\n",
    "  )\n",
    ")\n",
    "\n",
    "register_args = model.register(content_types=[\"text/csv\"], response_types=[\"text/csv\"],\n",
    "                          inference_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"], transform_instances=[\"ml.m5.large\"],\n",
    "                          model_package_group_name=\"customer-churn-models\", approval_status=\"Approved\",\n",
    "                               model_metrics=model_metrics,\n",
    "                          description=\"XGBoost model for customer churn prediction\")\n",
    "step_register_model = ModelStep(\n",
    "    name=\"RegisterXGBoostModel\",\n",
    "    step_args=register_args\n",
    ")\n",
    "\n",
    "print(\"âœ… Model registration step defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Create and Execute the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline with fixed name for versioning\n",
    "# SageMaker Pipelines now support versioning - use fixed names instead of timestamps\n",
    "pipeline_name = \"customer-churn-pipeline\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        train_test_split,\n",
    "        training_instance_type,\n",
    "        max_depth,\n",
    "        num_round\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "        step_evaluate,\n",
    "        step_create_model,\n",
    "        step_register_model\n",
    "    ],\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "print(f\"ðŸš€ Pipeline Name: {pipeline_name}\")\n",
    "print(f\"ðŸ“Š Pipeline Steps: {len(pipeline.steps)}\")\n",
    "print(\"â„¹ï¸  Using fixed name - SageMaker will create versions automatically\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Validate Pipeline Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "print('cwd', os.getcwd())\n",
    "# Validate the pipeline definition\n",
    "pipeline_definition = json.loads(pipeline.definition())\n",
    "print(\"Pipeline definition validated successfully!\")\n",
    "print(f\"\\nPipeline has {len(pipeline_definition['Steps'])} steps:\")\n",
    "for step in pipeline_definition['Steps']:\n",
    "    print(f\"  - {step['Name']}: {step['Type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Create/Update and Execute Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/update the pipeline (creates new version if pipeline exists)\n",
    "response = pipeline.upsert(role_arn=lab_session.role)\n",
    "print(f\"âœ… Pipeline '{pipeline_name}' created/updated successfully\")\n",
    "\n",
    "# Check if this created a new version\n",
    "try:\n",
    "    versions = pipeline.list_pipeline_versions()\n",
    "    version_count = len(versions)\n",
    "    latest_version = versions[0]['PipelineVersion'] if versions else 1\n",
    "    print(f\"ðŸ“‹ Pipeline now has {version_count} version(s), latest: v{latest_version}\")\n",
    "except:\n",
    "    print(\"ðŸ“‹ Version information not available\")\n",
    "\n",
    "# Start pipeline execution\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"ProcessingInstanceType\": \"ml.m5.large\",\n",
    "        \"TrainingInstanceType\": \"ml.m5.large\", \n",
    "        \"TrainTestSplit\": 0.33,\n",
    "        \"MaxDepth\": \"5\",\n",
    "        \"NumRound\": \"100\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Pipeline execution started\")\n",
    "print(f\"ðŸ“ Execution ARN: {execution.arn}\")\n",
    "print(f\"ðŸ“Š Status: {execution.describe()['PipelineExecutionStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Monitor Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor execution status\n",
    "execution.wait()\n",
    "# while True:\n",
    "#     status = execution.describe()['PipelineExecutionStatus']\n",
    "#     print(f\"Pipeline Status: {status}\")\n",
    "#\n",
    "#     if status in ['Succeeded', 'Failed', 'Stopped']:\n",
    "#         break\n",
    "#\n",
    "#     # Check step statuses\n",
    "#     steps = execution.list_steps()\n",
    "#     for step in steps:\n",
    "#         print(f\"  - {step['StepName']}: {step['StepStatus']}\")\n",
    "#\n",
    "#     time.sleep(30)\n",
    "#     print(\"---\")\n",
    "#\n",
    "# print(f\"\\nâœ… Pipeline execution completed with status: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Retrieve Pipeline Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get execution steps details\n",
    "execution_steps = execution.list_steps()\n",
    "\n",
    "for step in execution_steps:\n",
    "    print(f\"\\nStep: {step['StepName']}\")\n",
    "    print(f\"  Status: {step['StepStatus']}\")\n",
    "\n",
    "    if step['StepName'] == 'TrainXGBoostModel' and step['StepStatus'] == 'Succeeded':\n",
    "        # Get training job details\n",
    "        training_job_arn = step['Metadata']['TrainingJob']['Arn']\n",
    "        print(f\"  Training Job ARN: {training_job_arn}\")\n",
    "\n",
    "    elif step['StepName'] == 'CreateXGBoostModel' and step['StepStatus'] == 'Succeeded':\n",
    "        # Get model details\n",
    "        model_arn = step['Metadata']['Model']['Arn']\n",
    "        print(f\"  Model ARN: {model_arn}\")\n",
    "\n",
    "    elif step['StepName'] == 'EvaluateModel' and step['StepStatus'] == 'Succeeded':\n",
    "        outputs = step['Metadata']['ProcessingJob']['ProcessingOutputConfig']['Outputs']\n",
    "        eval_uri = next((o['S3Output']['S3Uri'] for o in outputs if o['OutputName'] == 'evaluation'), None)\n",
    "        if eval_uri:\n",
    "            print(f\"  Evaluation report: {eval_uri}/evaluation.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## View Pipeline Execution in SageMaker Studio\n",
    "\n",
    "You can also view and manage your pipeline execution in SageMaker Studio:\n",
    "1. Open SageMaker Studio\n",
    "2. Navigate to the Pipelines section\n",
    "3. Select your pipeline to view execution details, logs, and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Pipeline Version Management (Optional)\n",
    "\n",
    "With SageMaker Pipeline versioning, you can manage different versions of your pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all versions of the pipeline\n",
    "try:\n",
    "    versions = pipeline.list_pipeline_versions()\n",
    "    print(f\"ðŸ“‹ Pipeline '{pipeline_name}' versions:\")\n",
    "    for version in versions[:5]:  # Show last 5 versions\n",
    "        print(f\"  - Version {version['PipelineVersion']}: Created {version['CreationTime']}\")\n",
    "        \n",
    "    if len(versions) > 5:\n",
    "        print(f\"  ... and {len(versions) - 5} more versions\")\n",
    "        \n",
    "    # Show how to execute a specific version\n",
    "    print(f\"\\nðŸ’¡ To execute a specific version:\")\n",
    "    print(f\"   execution = pipeline.start(pipeline_version=1, parameters={{...}})\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Could not retrieve version information: {e}\")\n",
    "    print(\"This may be normal for newly created pipelines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Clean Up Resources (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete the pipeline (uncomment to execute)\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"âœ… Pipeline '{pipeline_name}' deleted\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
