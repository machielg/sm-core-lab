{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Install SageMaker core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This notebook assumes a virtual environment containing sagemaker-core. In SageMaker studio start a terminal and run `uv pip install -e .` from the git project root:\n",
    "```\n",
    "sagemaker-user@default:~/sm-core-lab/uv pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_INSTANCE = \"ml.m5.large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "lab_session = CoreLabSession('xgboost', 'customer-churn', default_folder='core_notebook', create_run_folder=True, aws_profile='sagemaker-role')\n",
    "lab_session.print()\n",
    "core_session = lab_session.core_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "data = core_session.read_s3_file(f\"sagemaker-example-files-prod-{lab_session.region}\", \"datasets/tabular/synthetic/churn.txt\")\n",
    "\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "# Phone number is unique - will not add value to classifier\n",
    "df = df.drop(\"Phone\", axis=1)\n",
    "\n",
    "# Cast Area Code to non-numeric\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "# Remove one feature from highly corelated pairs\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# One-hot encode catagorical features into numeric features\n",
    "model_data = pd.get_dummies(df)\n",
    "model_data = pd.concat(\n",
    "    [\n",
    "        model_data[\"Churn?_True.\"],\n",
    "        model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "model_data = model_data.astype(float)\n",
    "\n",
    "# Split data into train and validation datasets\n",
    "train_data, validation_data = train_test_split(model_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# Further split the validation dataset into test and validation datasets.\n",
    "validation_data, test_data = train_test_split(validation_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# Remove and store the target column for the test data. This is used for calculating performance metrics after training, on unseen data.\n",
    "test_target_column = test_data[\"Churn?_True.\"]\n",
    "test_data.drop([\"Churn?_True.\"], axis=1, inplace=True)\n",
    "\n",
    "# Store all datasets locally\n",
    "train_data.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=False, index=False)\n",
    "test_data.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "# Upload each dataset to S3\n",
    "s3_train_input = core_session.upload_data(\"train.csv\")\n",
    "s3_validation_input = core_session.upload_data(\"validation.csv\")\n",
    "s3_test_input = core_session.upload_data(\"test.csv\")\n",
    "\n",
    "print(\"Datasets uploaded to:\")\n",
    "print(s3_train_input)\n",
    "print(s3_validation_input)\n",
    "print(s3_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = lab_session.retrieve_image('1.7-1')\n",
    "print(\"Using image:\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Generic shapes for Training and Tuning jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Set up configuration for training and tuning jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.shapes import AlgorithmSpecification, OutputDataConfig, DataSource, S3DataSource, Channel, ResourceConfig\n",
    "\n",
    "algorithm_spec = AlgorithmSpecification(training_image=image, training_input_mode=\"File\")\n",
    "\n",
    "channel_train = Channel(\n",
    "            channel_name=\"train\",\n",
    "            content_type=\"csv\",\n",
    "            data_source=DataSource(\n",
    "                s3_data_source=S3DataSource(\n",
    "                    s3_data_type=\"S3Prefix\",\n",
    "                    s3_uri=s3_train_input,\n",
    "                    s3_data_distribution_type=\"FullyReplicated\")))\n",
    "\n",
    "channel_validation = Channel(\n",
    "            channel_name=\"validation\",\n",
    "            content_type=\"csv\",\n",
    "            data_source=DataSource(\n",
    "                s3_data_source=S3DataSource(\n",
    "                    s3_data_type=\"S3Prefix\",\n",
    "                    s3_uri=s3_validation_input,\n",
    "                    s3_data_distribution_type=\"FullyReplicated\")))\n",
    "\n",
    "output_data_config = OutputDataConfig(s3_output_path=lab_session.jobs_output_s3_uri)\n",
    "\n",
    "training_instance_config = ResourceConfig(instance_type=DEFAULT_INSTANCE, instance_count=1, volume_size_in_gb=30) # keep_alive_period_in_seconds=60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import TrainingJob\n",
    "from sagemaker_core.shapes import StoppingCondition\n",
    "\n",
    "# Specify hyperparameters\n",
    "HYPER_PARAMS = {\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.8\",\n",
    "    \"verbosity\": \"0\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": \"100\",\n",
    "}\n",
    "\n",
    "# Create training job.\n",
    "training_job = TrainingJob.create(\n",
    "    training_job_name=lab_session.training_job_name,\n",
    "    hyper_parameters=HYPER_PARAMS,\n",
    "    algorithm_specification=algorithm_spec,\n",
    "    role_arn=lab_session.role,\n",
    "    input_data_config=[\n",
    "        channel_train,\n",
    "        channel_validation\n",
    "    ],\n",
    "    output_data_config=output_data_config,\n",
    "    resource_config=training_instance_config,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=600),\n",
    "    session=lab_session.core_session.boto_session,\n",
    "    region=lab_session.region\n",
    ")\n",
    "\n",
    "# Wait for the training job to complete\n",
    "training_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model artifacts:\", training_job.model_artifacts.s3_model_artifacts)\n",
    "print(\"Training job ARN:\", training_job.training_job_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Assignment 2 - Hyper Paramater Tuning (try the assignment first!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import HyperParameterTuningJob\n",
    "from sagemaker_core.shapes import (\n",
    "    HyperParameterTuningJobConfig,\n",
    "    ResourceLimits,\n",
    "    ParameterRanges,\n",
    "    AutoParameter,\n",
    "    Autotune,\n",
    "    HyperParameterTrainingJobDefinition,\n",
    "    HyperParameterTuningJobObjective,\n",
    "    HyperParameterAlgorithmSpecification,\n",
    "    StoppingCondition,\n",
    ")\n",
    "\n",
    "algo_spec = HyperParameterAlgorithmSpecification(training_image=image, training_input_mode=\"File\")\n",
    "\n",
    "# Create HyperParameterTrainingJobDefinition object, setting the core training job  \n",
    "hyper_parameter_training_job_defintion = HyperParameterTrainingJobDefinition(\n",
    "    role_arn=lab_session.role,\n",
    "    algorithm_specification=algo_spec,\n",
    "    input_data_config=[\n",
    "        channel_train,\n",
    "        channel_validation\n",
    "    ],\n",
    "    output_data_config=output_data_config,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=900),  # per job timout\n",
    "    resource_config=training_instance_config,\n",
    "    static_hyper_parameters={\"objective\": \"binary:logistic\"}\n",
    ")\n",
    "\n",
    "# Create HyperParameterTrainingJobDefinition object, setting the tuning strategy\n",
    "tuning_job_config = HyperParameterTuningJobConfig(\n",
    "    strategy=\"Bayesian\",\n",
    "    hyper_parameter_tuning_job_objective=HyperParameterTuningJobObjective(\n",
    "        type=\"Maximize\", metric_name=\"validation:auc\"\n",
    "    ),\n",
    "    resource_limits=ResourceLimits(\n",
    "        max_number_of_training_jobs=10,\n",
    "        max_parallel_training_jobs=5,\n",
    "        max_runtime_in_seconds=3600,  # total timeout\n",
    "    ),\n",
    "    training_job_early_stopping_type=\"Auto\",\n",
    "    parameter_ranges=ParameterRanges(\n",
    "        auto_parameters=[\n",
    "            AutoParameter(name=\"max_depth\", value_hint=\"5\"),\n",
    "            AutoParameter(name=\"eta\", value_hint=\"0.1\"),\n",
    "            AutoParameter(name=\"gamma\", value_hint=\"8\"),\n",
    "            AutoParameter(name=\"min_child_weight\", value_hint=\"2\"),\n",
    "            AutoParameter(name=\"subsample\", value_hint=\"0.5\"),\n",
    "            AutoParameter(name=\"num_round\", value_hint=\"50\"),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Create the tuning job using the 2 configuration objects above\n",
    "tuning_job = HyperParameterTuningJob.create(\n",
    "    hyper_parameter_tuning_job_name=lab_session.tuning_job_name,\n",
    "    autotune=Autotune(mode=\"Enabled\"),\n",
    "    training_job_definition=hyper_parameter_training_job_defintion,\n",
    "    hyper_parameter_tuning_job_config=tuning_job_config,\n",
    ")\n",
    "\n",
    "tuning_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best training job name\n",
    "best_job_name = tuning_job.best_training_job.training_job_name\n",
    "metric = tuning_job.best_training_job.final_hyper_parameter_tuning_job_objective_metric\n",
    "print(f\"Best training job: {best_job_name}\")\n",
    "print(\"Objective metric:\", metric.metric_name, \"=\", metric.value)\n",
    "# Get the best training job object\n",
    "best_training_job = TrainingJob.get(best_job_name)\n",
    "\n",
    "# Print the hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "for param_name, param_value in best_training_job.hyper_parameters.items():\n",
    "    print(f\"  {param_name}: {param_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "We assume one model is created/updated. A new model version/name would be set explicitly either as the next production release or a 'candidate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import Model\n",
    "from sagemaker_core.shapes import ContainerDefinition\n",
    "from corelab.core.utils import try_delete\n",
    "\n",
    "model_s3_uri = training_job.model_artifacts.s3_model_artifacts  # Get URI of model artifacts from the training job.\n",
    "# model_s3_uri = best_training_job.model_artifacts.s3_model_artifacts  # Get URI of model artifacts of the best model from the tuning job.\n",
    "\n",
    "try_delete(Model, lab_session.model_name)\n",
    "\n",
    "# Create SageMaker model: An image along with the model artifact to use.\n",
    "customer_churn_model = Model.create(\n",
    "    model_name=lab_session.model_name,\n",
    "    primary_container=ContainerDefinition(image=image, model_data_url=model_s3_uri),\n",
    "    execution_role_arn=lab_session.role,\n",
    ")\n",
    "\n",
    "print(customer_churn_model.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import TransformJob\n",
    "from sagemaker_core.shapes import (\n",
    "    TransformInput,\n",
    "    TransformDataSource,\n",
    "    TransformS3DataSource,\n",
    "    TransformOutput,\n",
    "    TransformResources,\n",
    ")\n",
    "\n",
    "transform_input = TransformInput(\n",
    "        data_source=TransformDataSource(\n",
    "            s3_data_source=TransformS3DataSource(s3_data_type=\"S3Prefix\", s3_uri=s3_test_input)\n",
    "        ),\n",
    "        content_type=\"text/csv\")\n",
    "\n",
    "transform_output = TransformOutput(s3_output_path=lab_session.transform_output_s3_uri)\n",
    "\n",
    "transform_resource = TransformResources(\n",
    "        instance_type=DEFAULT_INSTANCE, instance_count=1\n",
    "    )\n",
    "\n",
    "# Create Transform Job.\n",
    "transform_job = TransformJob.create(\n",
    "    transform_job_name=lab_session.transform_job_name,\n",
    "    model_name=customer_churn_model.get_name(),\n",
    "    transform_input=transform_input,\n",
    "    transform_output=transform_output,\n",
    "    transform_resources=transform_resource,\n",
    ")\n",
    "\n",
    "transform_job.wait()\n",
    "\n",
    "print(\"Tranform done, find the output at:\", transform_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_output = TransformOutput(s3_output_path=lab_session.transform_output_s3_uri)\n",
    "print(\"Tranform done, find the output at:\", transform_output.s3_output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Endpoint creation\n",
    "\n",
    "We assume one hosted endpoint will exist for the churn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import Endpoint, EndpointConfig\n",
    "from sagemaker_core.shapes import ProductionVariant\n",
    "from corelab.core.utils import try_delete\n",
    "\n",
    "try_delete(EndpointConfig, lab_session.endpoint_config_name)\n",
    "\n",
    "endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=lab_session.endpoint_config_name,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"AllTraffic\",\n",
    "            model_name=customer_churn_model.get_name(),\n",
    "            instance_type=DEFAULT_INSTANCE,\n",
    "            initial_instance_count=1,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "try_delete(Endpoint, lab_session.endpoint_name)\n",
    "sagemaker_endpoint = Endpoint.create(\n",
    "    endpoint_name=lab_session.endpoint_name,\n",
    "    endpoint_config_name=endpoint_config.get_name(),\n",
    ")\n",
    "sagemaker_endpoint.wait_for_status(target_status=\"InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Provisioned vs Serverless Endpoints\n",
    "\n",
    "The above creates a **provisioned endpoint** with dedicated instances that are always running. \n",
    "\n",
    "**Provisioned Endpoints:**\n",
    "- ✅ Predictable performance and latency\n",
    "- ✅ Good for consistent, high-volume traffic\n",
    "- ❌ Always billing (even when idle)\n",
    "- ❌ Fixed capacity\n",
    "\n",
    "**Serverless Endpoints:**\n",
    "- ✅ Pay only for actual inference time\n",
    "- ✅ Automatic scaling (0 to max concurrency)\n",
    "- ✅ No infrastructure management\n",
    "- ❌ Cold start latency (first request)\n",
    "- ❌ Limited to 6GB memory max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Serverless Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import Endpoint, EndpointConfig\n",
    "from sagemaker_core.shapes import ProductionVariant, ProductionVariantServerlessConfig\n",
    "\n",
    "# patch bug in sagemaker core\n",
    "import sagemaker_core.main.utils as smutils\n",
    "smutils.SPECIAL_SNAKE_TO_PASCAL_MAPPINGS = {\n",
    "    \"volume_size_in_g_b\": \"VolumeSizeInGB\",\n",
    "    \"volume_size_in_gb\": \"VolumeSizeInGB\",\n",
    "    \"memory_size_in_mb\": \"MemorySizeInMB\",\n",
    "    \"supported_response_mime_types\": \"SupportedResponseMIMETypes\",\n",
    "}\n",
    "# end patch\n",
    "\n",
    "try_delete(EndpointConfig, lab_session.serverless_endpoint_config_name)\n",
    "\n",
    "# Create serverless endpoint configuration\n",
    "serverless_endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=lab_session.serverless_endpoint_config_name,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"ServerlessVariant\",\n",
    "            model_name=customer_churn_model.get_name(),\n",
    "            serverless_config=ProductionVariantServerlessConfig(\n",
    "                memory_size_in_mb=2048,    # 2GB memory (valid: 1024, 2048, 3072, 4096, 5120, 6144)\n",
    "                max_concurrency=10,        # Handle up to 10 concurrent requests\n",
    "                provisioned_concurrency=1,  # Keep 1 instance warm (optional),\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "try_delete(Endpoint, lab_session.serverless_endpoint_name)\n",
    "\n",
    "# Create serverless endpoint\n",
    "serverless_endpoint = Endpoint.create(\n",
    "    endpoint_name=lab_session.serverless_endpoint_name,\n",
    "    endpoint_config_name=serverless_endpoint_config.get_name(),\n",
    ")\n",
    "serverless_endpoint.wait_for_status(target_status=\"InService\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Invoke endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def print_performance_metrics(probs, y, threshold=0.5):\n",
    "    \"\"\"Lightweight method for printing performance metrics\"\"\"\n",
    "\n",
    "    predictions = (probs >= threshold).astype(int)\n",
    "\n",
    "    # Compare predictions with the stored target\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "    roc_auc = roc_auc_score(y, probs)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Create a predictor with automatic CSV handling\n",
    "predictor = Predictor(\n",
    "  endpoint_name=serverless_endpoint.endpoint_name,\n",
    "  serializer=CSVSerializer(),\n",
    "  deserializer=CSVDeserializer()\n",
    ")\n",
    "\n",
    "# Much cleaner!\n",
    "result = predictor.predict(test_data)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 🧹 Cleanup Resources\n",
    "\n",
    "To avoid unnecessary AWS charges, it's important to delete all created resources when you're finished with this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete both endpoints (this will take a few minutes)\n",
    "\n",
    "print(\"🗑️  Deleting provisioned endpoint...\")\n",
    "try:\n",
    "    sagemaker_endpoint.delete()\n",
    "    sagemaker_endpoint.wait_for_delete()\n",
    "    print(\"✅ Provisioned endpoint deletion initiated\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting provisioned endpoint: {e}\")\n",
    "\n",
    "# Delete endpoint configurations\n",
    "print(\"\\n🗑️  Deleting endpoint configurations...\")\n",
    "try:\n",
    "    endpoint_config.delete()\n",
    "    print(\"✅ Provisioned endpoint config deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting provisioned endpoint config: {e}\")\n",
    "\n",
    "try:\n",
    "    serverless_endpoint_config.delete()\n",
    "    print(\"✅ Serverless endpoint config deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting serverless endpoint config: {e}\")\n",
    "\n",
    "print(\"\\n✨ Cleanup completed!\")\n",
    "print(\"\\n💰 Note: S3 storage costs will continue until you manually delete the bucket contents\")\n",
    "print(f\"    Bucket location: {lab_session.base_s3_uri}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
