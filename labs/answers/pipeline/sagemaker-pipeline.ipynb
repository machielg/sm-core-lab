{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SageMaker Core Pipeline - Data Prep, Training, and Model Creation\n",
    "\n",
    "This notebook demonstrates how to create a complete ML pipeline using SageMaker Core that includes:\n",
    "1. Data Processing - Prepare and split the customer churn dataset\n",
    "2. Model Training - Train an XGBoost model on processed data  \n",
    "3. Model Creation - Create a deployable SageMaker model from training artifacts"
   ],
   "id": "c577e5a382ec2cd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "15efd24b255f1468"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize CoreLab Session",
   "id": "e76ffb9d6fca7539"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "lab_session = CoreLabSession(\n",
    "    'xgboost', \n",
    "    'customer-churn-pipeline', \n",
    "    default_folder='pipeline_notebook', \n",
    "    create_run_folder=True, \n",
    "    aws_profile='sagemaker-role'\n",
    ")\n",
    "lab_session.print()\n",
    "core_session = lab_session.core_session"
   ],
   "id": "5adbfbf3ba5a07ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Import SageMaker Pipeline Components\n",
    "\n",
    "Note: SageMaker Pipelines SDK (not sagemaker-core) is used for pipeline orchestration."
   ],
   "id": "f8a1b5cda9f6c6ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Pipeline-specific imports from SageMaker SDK\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterFloat,\n",
    "    ParameterString\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "# Processing imports\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Training imports  \n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Model imports\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.model_metrics import ModelMetrics, MetricsSource\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"All pipeline modules imported successfully\")"
   ],
   "id": "62410b767c9cc480"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Upload Processing Script",
   "id": "5e4ee0283a70ee3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Upload the preprocessing script\n",
    "code_s3_uri = lab_session.upload_file('src', 'preprocessing.py', 'pipeline-code')\n",
    "\n",
    "# Define data locations\n",
    "data_s3_uri = f\"s3://sagemaker-example-files-prod-{lab_session.region}/datasets/tabular/synthetic/churn.txt\"\n",
    "pipeline_output_s3_uri = lab_session.jobs_output_s3_uri\n",
    "\n",
    "print(f\"üìÇ Code S3 URI: {code_s3_uri}\")\n",
    "print(f\"üìÅ Data S3 URI: {data_s3_uri}\")\n",
    "print(f\"üì§ Pipeline Output S3 URI: {pipeline_output_s3_uri}\")"
   ],
   "id": "8c6bd50dd386b342"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define Pipeline Parameters\n",
    "\n",
    "Pipeline parameters allow us to customize pipeline executions without modifying the code."
   ],
   "id": "a8b24b715fcdfec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define pipeline parameters for flexibility\n",
    "\n",
    "# Processing parameters\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "train_test_split = ParameterFloat(\n",
    "    name=\"TrainTestSplit\",\n",
    "    default_value=0.33\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=\"ml.m5.large\"\n",
    ")\n",
    "\n",
    "max_depth = ParameterString(\n",
    "    name=\"MaxDepth\",\n",
    "    default_value=\"5\"\n",
    ")\n",
    "\n",
    "num_round = ParameterString(\n",
    "    name=\"NumRound\",\n",
    "    default_value=\"100\"\n",
    ")\n",
    "\n",
    "print(\"Pipeline parameters defined\")"
   ],
   "id": "7b6066075ff3142"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define Processing Step\n",
    "\n",
    "This step processes raw data and splits it into train, validation, and test sets."
   ],
   "id": "ea1d2b4c4630160b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a ScriptProcessor\n",
    "script_processor = ScriptProcessor(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=1,\n",
    "    role=lab_session.role,\n",
    "    sagemaker_session=lab_session.core_session\n",
    ")\n",
    "\n",
    "# Define the processing step\n",
    "step_process = ProcessingStep(\n",
    "    name=\"PreprocessCustomerChurnData\",\n",
    "    processor=script_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=data_s3_uri,\n",
    "            destination=\"/opt/ml/processing/input/data\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=f\"{pipeline_output_s3_uri}/data/train\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation\",\n",
    "            source=\"/opt/ml/processing/output/validation\",\n",
    "            destination=f\"{pipeline_output_s3_uri}/data/validation\"\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=f\"{pipeline_output_s3_uri}/data/test\"\n",
    "        )\n",
    "    ],\n",
    "    code=code_s3_uri,\n",
    "    job_arguments=[\"--train-test-split\", train_test_split]\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Processing step defined\")"
   ],
   "id": "57ddf55d0bfe138a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: Define Training Step\n",
    "\n",
    "This step trains an XGBoost model using the processed data from Step 1."
   ],
   "id": "35c383809a789860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create XGBoost estimator\n",
    "xgboost_estimator = Estimator(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    role=lab_session.role,\n",
    "    output_path=f\"{pipeline_output_s3_uri}/models\",\n",
    "    sagemaker_session=lab_session.sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"max_depth\": max_depth,\n",
    "        \"eta\": \"0.2\",\n",
    "        \"gamma\": \"4\",\n",
    "        \"min_child_weight\": \"6\",\n",
    "        \"subsample\": \"0.8\",\n",
    "        \"verbosity\": \"0\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_round\": num_round\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define training step with dependencies on processing outputs\n",
    "step_train = TrainingStep(\n",
    "    name=\"TrainXGBoostModel\",\n",
    "    estimator=xgboost_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training step defined\")"
   ],
   "id": "f073e34a1c38bf4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Define Model Creation Step\n",
    "\n",
    "This step creates a SageMaker Model from the trained model artifacts."
   ],
   "id": "87f325223b322ae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a Model object\n",
    "model = Model(\n",
    "    image_uri=lab_session.retrieve_image('1.7-1'),\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    role=lab_session.role,\n",
    "    sagemaker_session=lab_session.sagemaker_session\n",
    ")\n",
    "\n",
    "# Define model creation step\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateXGBoostModel\",\n",
    "    step_args=model.create(\n",
    "        instance_type=\"ml.m5.large\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model creation step defined\")"
   ],
   "id": "44776ea9b47511f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optional: Model Registry Step\n",
    "\n",
    "Register the model in SageMaker Model Registry for versioning and deployment management."
   ],
   "id": "e1bd9523304b434a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define model metrics (optional)\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=f\"{pipeline_output_s3_uri}/evaluation/statistics.json\",\n",
    "        content_type=\"application/json\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Register model step\n",
    "step_register_model = RegisterModel(\n",
    "    name=\"RegisterXGBoostModel\",\n",
    "    estimator=xgboost_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=\"customer-churn-models\",\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model registration step defined\")"
   ],
   "id": "d33d215c49688b4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create and Execute the Pipeline",
   "id": "49be7fdc950137f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create the pipeline with all steps\n",
    "pipeline_name = f\"customer-churn-pipeline-{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        train_test_split,\n",
    "        training_instance_type,\n",
    "        max_depth,\n",
    "        num_round\n",
    "    ],\n",
    "    steps=[\n",
    "        step_process,\n",
    "        step_train,\n",
    "        step_create_model,\n",
    "        # step_register_model  # Uncomment to include model registry\n",
    "    ],\n",
    "    sagemaker_session=lab_session.sagemaker_session\n",
    ")\n",
    "\n",
    "print(f\"üöÄ Pipeline Name: {pipeline_name}\")\n",
    "print(f\"üìä Pipeline Steps: {len(pipeline.steps)}\")"
   ],
   "id": "763a931bee96423c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validate Pipeline Definition",
   "id": "d47757da62eedc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Validate the pipeline definition\n",
    "pipeline_definition = pipeline.definition()\n",
    "print(\"Pipeline definition validated successfully!\")\n",
    "print(f\"\\nPipeline has {len(pipeline_definition['Steps'])} steps:\")\n",
    "for step in pipeline_definition['Steps']:\n",
    "    print(f\"  - {step['Name']}: {step['Type']}\")"
   ],
   "id": "3bbdecdc9c35ae98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create/Update and Execute Pipeline",
   "id": "e27f1f9713a34746"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create or update the pipeline\n",
    "pipeline.upsert(role_arn=lab_session.role)\n",
    "\n",
    "print(f\"‚úÖ Pipeline '{pipeline_name}' created/updated successfully\")"
   ],
   "id": "c507bc74cf72bfa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Start pipeline execution\n",
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"ProcessingInstanceType\": \"ml.m5.large\",\n",
    "        \"TrainingInstanceType\": \"ml.m5.large\",\n",
    "        \"TrainTestSplit\": 0.33,\n",
    "        \"MaxDepth\": \"5\",\n",
    "        \"NumRound\": \"100\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"üöÄ Pipeline execution started\")\n",
    "print(f\"üìù Execution ARN: {execution.arn}\")\n",
    "print(f\"üìä Status: {execution.describe()['PipelineExecutionStatus']}\")"
   ],
   "id": "6b751fede2e45938"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Monitor Pipeline Execution",
   "id": "34a5e677045a7b82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Monitor execution status\n",
    "\n",
    "while True:\n",
    "    status = execution.describe()['PipelineExecutionStatus']\n",
    "    print(f\"Pipeline Status: {status}\")\n",
    "    \n",
    "    if status in ['Succeeded', 'Failed', 'Stopped']:\n",
    "        break\n",
    "        \n",
    "    # Check step statuses\n",
    "    steps = execution.list_steps()\n",
    "    for step in steps:\n",
    "        print(f\"  - {step['StepName']}: {step['StepStatus']}\")\n",
    "    \n",
    "    time.sleep(30)\n",
    "    print(\"---\")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline execution completed with status: {status}\")"
   ],
   "id": "7dba6cd26c9b0b0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Retrieve Pipeline Outputs",
   "id": "56c38d43056c7de9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get execution steps details\n",
    "execution_steps = execution.list_steps()\n",
    "\n",
    "for step in execution_steps:\n",
    "    print(f\"\\nStep: {step['StepName']}\")\n",
    "    print(f\"  Status: {step['StepStatus']}\")\n",
    "    \n",
    "    if step['StepName'] == 'TrainXGBoostModel' and step['StepStatus'] == 'Succeeded':\n",
    "        # Get training job details\n",
    "        training_job_arn = step['Metadata']['TrainingJob']['Arn']\n",
    "        print(f\"  Training Job ARN: {training_job_arn}\")\n",
    "        \n",
    "    elif step['StepName'] == 'CreateXGBoostModel' and step['StepStatus'] == 'Succeeded':\n",
    "        # Get model details\n",
    "        model_arn = step['Metadata']['Model']['Arn']\n",
    "        print(f\"  Model ARN: {model_arn}\")"
   ],
   "id": "d07b2dcd40ab706c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## View Pipeline Execution in SageMaker Studio\n",
    "\n",
    "You can also view and manage your pipeline execution in SageMaker Studio:\n",
    "1. Open SageMaker Studio\n",
    "2. Navigate to the Pipelines section\n",
    "3. Select your pipeline to view execution details, logs, and metrics"
   ],
   "id": "7782f74faa1afc1e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Clean Up Resources (Optional)",
   "id": "690c17c7f211e6ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Delete the pipeline (uncomment to execute)\n",
    "# try:\n",
    "#     pipeline.delete()\n",
    "#     print(f\"‚úÖ Pipeline '{pipeline_name}' deleted\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting pipeline: {e}\")"
   ],
   "id": "13a280d62bccc3e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
