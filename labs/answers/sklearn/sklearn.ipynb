{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Training with a generic model library\n",
    "\n",
    "Using the scikit-learn image we have more control over the libraries installed and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "lab_session = CoreLabSession('sklearn', 'customer-churn', default_folder='sklearn_notebook', create_run_folder=True,\n",
    "                             aws_profile='sagemaker-role')\n",
    "lab_session.print()\n",
    "core_session = lab_session.core_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_INSTANCE = 'ml.m5.large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "\n",
    "data = core_session.read_s3_file(f\"sagemaker-example-files-prod-{lab_session.region}\",\n",
    "                                 \"datasets/tabular/synthetic/churn.txt\")\n",
    "\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "# Phone number is unique - will not add value to classifier\n",
    "df = df.drop(\"Phone\", axis=1)\n",
    "\n",
    "# Cast Area Code to non-numeric\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)\n",
    "\n",
    "# Remove one feature from highly corelated pairs\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# One-hot encode catagorical features into numeric features\n",
    "model_data = pd.get_dummies(df)\n",
    "model_data = pd.concat(\n",
    "    [\n",
    "        model_data[\"Churn?_True.\"],\n",
    "        model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "model_data = model_data.astype(float)\n",
    "\n",
    "# Split data into train and validation datasets\n",
    "train_data, validation_data = train_test_split(model_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# Further split the validation dataset into test and validation datasets.\n",
    "validation_data, test_data = train_test_split(validation_data, test_size=0.33, random_state=42)\n",
    "\n",
    "# Remove and store the target column for the test data. This is used for calculating performance metrics after training, on unseen data.\n",
    "test_target_column = test_data[\"Churn?_True.\"]\n",
    "test_data.drop([\"Churn?_True.\"], axis=1, inplace=True)\n",
    "\n",
    "# Store all datasets locally\n",
    "train_data.to_csv(\"train.csv\", header=True, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=True, index=False)\n",
    "test_data.to_csv(\"test.csv\", header=True, index=False)\n",
    "\n",
    "# Upload each dataset to S3\n",
    "s3_train_input = core_session.upload_data(\"train.csv\")\n",
    "s3_validation_input = core_session.upload_data(\"validation.csv\")\n",
    "s3_test_input = core_session.upload_data(\"test.csv\")\n",
    "\n",
    "print(\"Datasets uploaded to:\")\n",
    "print(s3_train_input)\n",
    "print(s3_validation_input)\n",
    "print(s3_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.shapes import AlgorithmSpecification, OutputDataConfig, DataSource, S3DataSource, Channel, \\\n",
    "    ResourceConfig\n",
    "\n",
    "image = lab_session.retrieve_image('1.2-1')\n",
    "\n",
    "print(\"Docker image:\", image)\n",
    "\n",
    "algorithm_spec = AlgorithmSpecification(training_image=image, training_input_mode=\"File\")\n",
    "\n",
    "channel_train = Channel(\n",
    "    channel_name=\"train\",\n",
    "    content_type=\"csv\",\n",
    "    data_source=DataSource(\n",
    "        s3_data_source=S3DataSource(\n",
    "            s3_data_type=\"S3Prefix\",\n",
    "            s3_uri=s3_train_input,\n",
    "            s3_data_distribution_type=\"FullyReplicated\")))\n",
    "\n",
    "channel_validation = Channel(\n",
    "    channel_name=\"validation\",\n",
    "    content_type=\"csv\",\n",
    "    data_source=DataSource(\n",
    "        s3_data_source=S3DataSource(\n",
    "            s3_data_type=\"S3Prefix\",\n",
    "            s3_uri=s3_validation_input,\n",
    "            s3_data_distribution_type=\"FullyReplicated\")))\n",
    "\n",
    "output_data_config = OutputDataConfig(s3_output_path=lab_session.jobs_output_s3_uri)\n",
    "\n",
    "training_instance_config = ResourceConfig(instance_type=DEFAULT_INSTANCE, instance_count=1,\n",
    "                                          volume_size_in_gb=30)  # keep_alive_period_in_seconds=60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.main.shapes import StoppingCondition\n",
    "from sagemaker_core.main.resources import TrainingJob\n",
    "from sagemaker import fw_utils\n",
    "\n",
    "!uv export --format=requirements.txt -o src/requirements.txt --no-dev -q --no-hashes\n",
    "\n",
    "print(\"Requirements exported to src/requirements.txt\")\n",
    "\n",
    "# Uploading code + requirements.txt\n",
    "upload_destination = lab_session.training_code_upload\n",
    "uploaded_code = fw_utils.tar_and_upload_dir(\n",
    "    session=lab_session.core_session.boto_session,  # or just sagemaker_session\n",
    "    bucket=upload_destination.bucket,\n",
    "    s3_key_prefix=upload_destination.prefix,\n",
    "    script='train.py',  # Entry point script\n",
    "    directory='./src'  # Directory containing your code\n",
    ")\n",
    "print(\"Uploaded code:\", uploaded_code)\n",
    "\n",
    "hyperparameters = {\n",
    "    'sagemaker_program': uploaded_code.script_name,  # 'train.py'\n",
    "    'sagemaker_submit_directory': uploaded_code.s3_prefix,  # S3 URI\n",
    "    'n_estimators': '100',\n",
    "    'max_depth': '10',\n",
    "    'target_column': 'Churn?_True.'\n",
    "}\n",
    "\n",
    "# Create training job.\n",
    "training_job = TrainingJob.create(\n",
    "    training_job_name=lab_session.training_job_name,\n",
    "    hyper_parameters=hyperparameters,\n",
    "    algorithm_specification=algorithm_spec,\n",
    "    role_arn=lab_session.role,\n",
    "    input_data_config=[\n",
    "        channel_train,\n",
    "        channel_validation\n",
    "    ],\n",
    "    output_data_config=output_data_config,\n",
    "    resource_config=training_instance_config,\n",
    "    stopping_condition=StoppingCondition(max_runtime_in_seconds=600),\n",
    "    session=lab_session.core_session.boto_session,\n",
    "    region=lab_session.region\n",
    ")\n",
    "\n",
    "# Wait for the training job to complete\n",
    "training_job.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker_core.resources import Model\n",
    "from sagemaker_core.shapes import ContainerDefinition\n",
    "from corelab.core.utils import try_delete\n",
    "\n",
    "!uv export --format=requirements.txt -o src/requirements.txt --no-dev -q --no-hashes\n",
    "\n",
    "model_s3_uri = training_job.model_artifacts.s3_model_artifacts  # Get URI of model artifacts from the training job.\n",
    "# model_s3_uri = best_training_job.model_artifacts.s3_model_artifacts  # Get URI of model artifacts of the best model from the tuning job.\n",
    "\n",
    "try_delete(Model, lab_session.model_name)\n",
    "\n",
    "infer_destination = lab_session.inference_code_upload\n",
    "uploaded_infer = fw_utils.tar_and_upload_dir(\n",
    "    lab_session.core_session.boto_session,\n",
    "    infer_destination.bucket,\n",
    "    infer_destination.prefix,\n",
    "    script='inference.py', directory='./src')\n",
    "# Create SageMaker model: An image along with the model artifact to use.\n",
    "customer_churn_model = Model.create(\n",
    "    model_name=lab_session.model_name,\n",
    "    primary_container=ContainerDefinition(image=image, model_data_url=model_s3_uri,\n",
    "                                          environment={\"SAGEMAKER_PROGRAM\": \"inference.py\", \"SAGEMAKER_SUBMIT_DIRECTORY\": uploaded_infer.s3_prefix}),\n",
    "    execution_role_arn=lab_session.role,\n",
    ")\n",
    "print(\"Model created:\", customer_churn_model.model_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corelab.core.utils import try_delete\n",
    "from sagemaker_core.resources import Endpoint, EndpointConfig\n",
    "from sagemaker_core.shapes import ProductionVariant, ProductionVariantServerlessConfig\n",
    "\n",
    "# patch bug in sagemaker core\n",
    "import sagemaker_core.main.utils as smutils\n",
    "\n",
    "smutils.SPECIAL_SNAKE_TO_PASCAL_MAPPINGS = {\n",
    "    \"volume_size_in_g_b\": \"VolumeSizeInGB\",\n",
    "    \"volume_size_in_gb\": \"VolumeSizeInGB\",\n",
    "    \"memory_size_in_mb\": \"MemorySizeInMB\",\n",
    "    \"supported_response_mime_types\": \"SupportedResponseMIMETypes\",\n",
    "}\n",
    "# end patch\n",
    "\n",
    "try_delete(EndpointConfig, lab_session.serverless_endpoint_config_name)\n",
    "\n",
    "# Create serverless endpoint configuration\n",
    "serverless_endpoint_config = EndpointConfig.create(\n",
    "    endpoint_config_name=lab_session.serverless_endpoint_config_name,\n",
    "    production_variants=[\n",
    "        ProductionVariant(\n",
    "            variant_name=\"ServerlessVariant\",\n",
    "            model_name=customer_churn_model.get_name(),\n",
    "            serverless_config=ProductionVariantServerlessConfig(\n",
    "                memory_size_in_mb=2048,  # 2GB memory (valid: 1024, 2048, 3072, 4096, 5120, 6144)\n",
    "                max_concurrency=10,  # Handle up to 10 concurrent requests\n",
    "                provisioned_concurrency=1  # Keep 1 instance warm (optional)\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "try_delete(Endpoint, lab_session.serverless_endpoint_name)\n",
    "\n",
    "# Create serverless endpoint\n",
    "serverless_endpoint = Endpoint.create(\n",
    "    endpoint_name=lab_session.serverless_endpoint_name,\n",
    "    endpoint_config_name=serverless_endpoint_config.get_name(),\n",
    ")\n",
    "serverless_endpoint.wait_for_status(target_status=\"InService\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "\n",
    "# Create a predictor with automatic CSV handling\n",
    "predictor = Predictor(\n",
    "  endpoint_name=serverless_endpoint.endpoint_name,\n",
    "  serializer=CSVSerializer(),\n",
    "  deserializer=CSVDeserializer()\n",
    ")\n",
    "\n",
    "# Much cleaner!\n",
    "result = predictor.predict(test_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
