{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker SDK 2025 Approach\n",
    "\n",
    "This notebook demonstrates the same customer churn prediction workflow as `core.ipynb` but using the modern SageMaker Python SDK for a much cleaner developer experience.\n",
    "\n",
    "**Key Benefits:**\n",
    "- 80% less code for the same functionality\n",
    "- High-level ML abstractions (Estimators, Predictors)\n",
    "- Automatic handling of AWS resource configuration\n",
    "- Clean inference with automatic serialization\n",
    "- Better error handling and debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "We'll use our existing `CoreLabSession` for session management but switch to SageMaker SDK for ML operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corelab.core.session import CoreLabSession\n",
    "\n",
    "# Use our custom session for authentication and S3 management\n",
    "lab_session = CoreLabSession('xgboost', 'customer-churn', \n",
    "                            default_folder='sagemaker_sdk_notebook', \n",
    "                            create_run_folder=True)\n",
    "lab_session.print()\n",
    "\n",
    "# Get SageMaker session for SDK integration\n",
    "sagemaker_session = lab_session.get_sagemaker_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Same data preparation as the core notebook - this part doesn't change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = lab_session.core_session.read_s3_file(\n",
    "    f\"sagemaker-example-files-prod-{lab_session.region}\", \n",
    "    \"datasets/tabular/synthetic/churn.txt\"\n",
    ")\n",
    "df = pd.read_csv(StringIO(data))\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Churn rate: {df['Churn?'].value_counts(normalize=True)['True.']:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "df = df.drop(\"Phone\", axis=1)  # Remove unique identifier\n",
    "df[\"Area Code\"] = df[\"Area Code\"].astype(object)  # Treat as categorical\n",
    "\n",
    "# Remove highly correlated features (charges are derived from minutes)\n",
    "df = df.drop([\"Day Charge\", \"Eve Charge\", \"Night Charge\", \"Intl Charge\"], axis=1)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "model_data = pd.get_dummies(df)\n",
    "\n",
    "# Move target to first column (XGBoost convention)\n",
    "model_data = pd.concat([\n",
    "    model_data[\"Churn?_True.\"],\n",
    "    model_data.drop([\"Churn?_False.\", \"Churn?_True.\"], axis=1),\n",
    "], axis=1)\n",
    "\n",
    "print(f\"Processed data shape: {model_data.shape}\")\n",
    "print(f\"Features: {model_data.shape[1] - 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation/test\n",
    "train_data, temp_data = train_test_split(model_data, test_size=0.33, random_state=42)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.33, random_state=42)\n",
    "\n",
    "print(f\"Train: {train_data.shape[0]} samples\")\n",
    "print(f\"Validation: {validation_data.shape[0]} samples\") \n",
    "print(f\"Test: {test_data.shape[0]} samples\")\n",
    "\n",
    "# Save and upload datasets\n",
    "train_data.to_csv(\"train.csv\", header=False, index=False)\n",
    "validation_data.to_csv(\"validation.csv\", header=False, index=False)\n",
    "\n",
    "# Store test target separately for evaluation\n",
    "test_target = test_data.iloc[:, 0]  # First column is target\n",
    "test_features = test_data.iloc[:, 1:]  # Rest are features\n",
    "test_features.to_csv(\"test.csv\", header=False, index=False)\n",
    "\n",
    "# Upload to S3\n",
    "s3_train_path = lab_session.core_session.upload_data(\"train.csv\")\n",
    "s3_validation_path = lab_session.core_session.upload_data(\"validation.csv\")\n",
    "s3_test_path = lab_session.core_session.upload_data(\"test.csv\")\n",
    "\n",
    "print(f\"\\nData uploaded to S3:\")\n",
    "print(f\"Train: {s3_train_path}\")\n",
    "print(f\"Validation: {s3_validation_path}\")\n",
    "print(f\"Test: {s3_test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with XGBoost Estimator\n",
    "\n",
    "This is where the SageMaker SDK shines - what took 50+ lines in sagemaker-core becomes just a few lines with the Estimator pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /Library/Application Support/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /Users/machiel/Library/Application Support/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:9                                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 </span>val_s3   = <span style=\"color: #808000; text-decoration-color: #808000\">\"s3://&lt;your-bucket&gt;/data/validation.csv\"</span>                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 # (Option A) Use simple InputData (string S3 URI is allowed)</span>                                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 9 train_input = InputData(channel_name=<span style=\"color: #808000; text-decoration-color: #808000\">\"train\"</span>, data_source=<span style=\"font-weight: bold; text-decoration: underline\">s3_train_path</span>)                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>val_input   = InputData(channel_name=<span style=\"color: #808000; text-decoration-color: #808000\">\"validation\"</span>, data_source=s3_validation_path)          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008700; text-decoration-color: #008700\">'s3_train_path'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:9                                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 6 \u001b[0mval_s3   = \u001b[33m\"\u001b[0m\u001b[33ms3://<your-bucket>/data/validation.csv\u001b[0m\u001b[33m\"\u001b[0m                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 7 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m# (Option A) Use simple InputData (string S3 URI is allowed)\u001b[0m                                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 9 train_input = InputData(channel_name=\u001b[33m\"\u001b[0m\u001b[33mtrain\u001b[0m\u001b[33m\"\u001b[0m, data_source=\u001b[1;4ms3_train_path\u001b[0m)                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m10 \u001b[0mval_input   = InputData(channel_name=\u001b[33m\"\u001b[0m\u001b[33mvalidation\u001b[0m\u001b[33m\"\u001b[0m, data_source=s3_validation_path)          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m11 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[38;2;0;135;0m's3_train_path'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.modules.train import ModelTrainer\n",
    "from sagemaker.modules.configs import Compute, InputData, Channel, S3DataSource, DataSource\n",
    "\n",
    "# S3 data (CSV or LIBSVM). XGBoost supports both.\n",
    "train_s3 = \"s3://<your-bucket>/data/train.csv\"\n",
    "val_s3   = \"s3://<your-bucket>/data/validation.csv\"\n",
    "\n",
    "# (Option A) Use simple InputData (string S3 URI is allowed)\n",
    "train_input = InputData(channel_name=\"train\", data_source=s3_train_path)\n",
    "val_input   = InputData(channel_name=\"validation\", data_source=s3_validation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "# Create XGBoost estimator - much cleaner than sagemaker-core!\n",
    "xgb_estimator = XGBoost(\n",
    "    framework_version='1.7-1',\n",
    "    role=lab_session.role,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    volume_size=30,\n",
    "    output_path=lab_session.base_s3_uri,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    base_job_name='xgboost-churn',\n",
    "    \n",
    "    # XGBoost hyperparameters\n",
    "    hyperparameters={\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'num_round': 100,\n",
    "        'verbosity': 0\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ XGBoost Estimator configured\")\n",
    "print(f\"Training will use: {xgb_estimator.instance_type}\")\n",
    "print(f\"Output location: {xgb_estimator.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model - just one line!\n",
    "# Compare this to the complex TrainingJob.create() in sagemaker-core\n",
    "\n",
    "xgb_estimator.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(f\"✅ Training completed!\")\n",
    "print(f\"Model artifacts: {xgb_estimator.model_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "The SageMaker SDK makes hyperparameter tuning much more intuitive with the `HyperparameterTuner` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    HyperparameterTuner,\n",
    "    IntegerParameter,\n",
    "    ContinuousParameter\n",
    ")\n",
    "\n",
    "# Define hyperparameter ranges - much cleaner than sagemaker-core!\n",
    "hyperparameter_ranges = {\n",
    "    'max_depth': IntegerParameter(3, 10),\n",
    "    'eta': ContinuousParameter(0.01, 0.3),\n",
    "    'gamma': ContinuousParameter(0, 5),\n",
    "    'min_child_weight': ContinuousParameter(1, 10),\n",
    "    'subsample': ContinuousParameter(0.5, 1.0),\n",
    "    'num_round': IntegerParameter(50, 200)\n",
    "}\n",
    "\n",
    "# Create tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    xgb_estimator,\n",
    "    objective_metric_name='validation:auc',\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=3,\n",
    "    base_tuning_job_name='xgboost-tuning'\n",
    ")\n",
    "\n",
    "print(\"✅ Hyperparameter tuner configured\")\n",
    "print(f\"Will run {tuner.max_jobs} tuning jobs\")\n",
    "print(f\"Optimizing: {tuner.objective_metric_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tuning - one line vs complex sagemaker-core setup!\n",
    "tuner.fit({\n",
    "    'train': s3_train_path,\n",
    "    'validation': s3_validation_path\n",
    "})\n",
    "\n",
    "print(\"✅ Hyperparameter tuning completed!\")\n",
    "\n",
    "# Get best training job details\n",
    "best_job = tuner.best_training_job()\n",
    "print(f\"Best job: {best_job['TrainingJobName']}\")\n",
    "print(f\"Best AUC: {best_job['FinalHyperParameterTuningJobObjectiveMetric']['Value']:.4f}\")\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"\\nBest hyperparameters:\")\n",
    "for key, value in best_job['TunedHyperParameters'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The SageMaker SDK makes deployment incredibly simple - just call `deploy()` on the tuner or estimator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the best model from tuning - this is so much cleaner!\n",
    "# Compare to the manual Model.create() + EndpointConfig.create() + Endpoint.create() in sagemaker-core\n",
    "\n",
    "predictor = tuner.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    endpoint_name=lab_session.endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"✅ Model deployed to endpoint: {predictor.endpoint_name}\")\n",
    "print(f\"Endpoint URL: {predictor.endpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serverless Deployment\n",
    "\n",
    "The SageMaker SDK also makes serverless deployment much simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serverless import ServerlessInferenceConfig\n",
    "\n",
    "# Deploy serverless endpoint - much cleaner than sagemaker-core!\n",
    "serverless_config = ServerlessInferenceConfig(\n",
    "    memory_size_in_mb=2048,\n",
    "    max_concurrency=10,\n",
    "    provisioned_concurrency=1\n",
    ")\n",
    "\n",
    "serverless_predictor = tuner.deploy(\n",
    "    serverless_inference_config=serverless_config,\n",
    "    endpoint_name=lab_session.serverless_endpoint_name\n",
    ")\n",
    "\n",
    "print(f\"✅ Serverless model deployed: {serverless_predictor.endpoint_name}\")\n",
    "print(f\"Memory: {serverless_config.memory_size_in_mb}MB\")\n",
    "print(f\"Max concurrency: {serverless_config.max_concurrency}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform\n",
    "\n",
    "The SageMaker SDK also simplifies batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create transformer - much simpler than sagemaker-core!\n",
    "transformer = tuner.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=lab_session.transform_output_s3_uri,\n",
    "    base_transform_job_name='xgboost-batch-transform'\n",
    ")\n",
    "\n",
    "# Run batch transform\n",
    "transformer.transform(\n",
    "    data=s3_test_path,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "print(f\"✅ Batch transform completed!\")\n",
    "print(f\"Results saved to: {transformer.output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Inference\n",
    "\n",
    "This is where the SageMaker SDK really shines - compare this clean interface to the fiddly `invoke()` + `read()` + `decode()` + `split()` in sagemaker-core!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# Test both endpoints with clean interface - no more fiddly response parsing!\n",
    "sample_data = test_features.head(10).values\n",
    "\n",
    "print(\"=== ENDPOINT COMPARISON ===\")\n",
    "print(f\"Testing with {len(sample_data)} samples\\n\")\n",
    "\n",
    "# Provisioned endpoint\n",
    "print(\"🖥️  PROVISIONED ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "provisioned_predictions = predictor.predict(sample_data)  # Clean and simple!\n",
    "provisioned_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(provisioned_predictions).shape}\")\n",
    "print(f\"   Latency: {provisioned_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {provisioned_predictions[:3]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Serverless endpoint  \n",
    "print(\"☁️  SERVERLESS ENDPOINT:\")\n",
    "start_time = time.time()\n",
    "serverless_predictions = serverless_predictor.predict(sample_data)  # Also clean!\n",
    "serverless_latency = (time.time() - start_time) * 1000\n",
    "\n",
    "print(f\"   Predictions shape: {np.array(serverless_predictions).shape}\")\n",
    "print(f\"   Latency: {serverless_latency:.1f}ms\")\n",
    "print(f\"   Sample predictions: {serverless_predictions[:3]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Compare results\n",
    "predictions_match = np.allclose(provisioned_predictions, serverless_predictions, rtol=1e-5)\n",
    "print(f\"✅ Predictions match: {predictions_match}\")\n",
    "print(f\"📊 Latency difference: {abs(serverless_latency - provisioned_latency):.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on full test set\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "\n",
    "# Get predictions for full test set\n",
    "test_predictions = predictor.predict(test_features.values)\n",
    "test_probabilities = np.array(test_predictions)\n",
    "test_binary = (test_probabilities >= 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_target, test_binary)\n",
    "precision = precision_score(test_target, test_binary)\n",
    "recall = recall_score(test_target, test_binary)\n",
    "auc = roc_auc_score(test_target, test_probabilities)\n",
    "\n",
    "print(f\"Test Set Performance:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  ROC AUC:   {auc:.4f}\")\n",
    "\n",
    "print(f\"\\n📊 Tested on {len(test_target)} samples\")\n",
    "print(f\"🎯 Churn rate in test set: {test_target.mean():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "The SageMaker SDK also makes cleanup simpler with built-in methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources - much simpler than sagemaker-core!\n",
    "\n",
    "print(\"🧹 Cleaning up resources...\")\n",
    "\n",
    "# Delete endpoints (predictors handle the cleanup automatically)\n",
    "print(\"\\n🗑️  Deleting endpoints...\")\n",
    "try:\n",
    "    predictor.delete_endpoint()\n",
    "    print(\"✅ Provisioned endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting provisioned endpoint: {e}\")\n",
    "\n",
    "try:\n",
    "    serverless_predictor.delete_endpoint()\n",
    "    print(\"✅ Serverless endpoint deleted\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error deleting serverless endpoint: {e}\")\n",
    "\n",
    "# Note: The SageMaker SDK automatically cleans up endpoint configs\n",
    "# and models when deleting endpoints (unless they're shared)\n",
    "\n",
    "print(\"\\n✨ Cleanup completed!\")\n",
    "print(\"\\n💰 Cost Summary:\")\n",
    "print(f\"   Training time: ~2-3 minutes\")\n",
    "print(f\"   Tuning time: ~10-15 minutes\")\n",
    "print(f\"   Inference time: ~5 minutes\")\n",
    "print(f\"   Storage location: {lab_session.base_s3_uri}\")\n",
    "print(\"\\n📝 Remember to delete S3 data when you're completely done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: SageMaker SDK vs sagemaker-core\n",
    "\n",
    "This notebook demonstrates the dramatic improvements in developer experience when using the SageMaker SDK:\n",
    "\n",
    "### Code Reduction:\n",
    "- **Training**: 50+ lines → 10 lines (80% reduction)\n",
    "- **Hyperparameter Tuning**: 40+ lines → 15 lines (70% reduction)  \n",
    "- **Deployment**: 30+ lines → 5 lines (85% reduction)\n",
    "- **Inference**: Fiddly response parsing → Clean `.predict()` calls\n",
    "\n",
    "### Developer Experience:\n",
    "- ✅ **Intuitive**: ML-focused abstractions (Estimators, Predictors)\n",
    "- ✅ **Less error-prone**: Automatic configuration and validation\n",
    "- ✅ **Cleaner inference**: No manual response parsing\n",
    "- ✅ **Better debugging**: Framework-specific error handling\n",
    "- ✅ **Local mode**: Test everything locally before deployment\n",
    "\n",
    "### When to use each:\n",
    "- **SageMaker SDK**: ML development, experimentation, production ML workflows\n",
    "- **sagemaker-core**: Infrastructure management, custom tooling, precise AWS API control\n",
    "\n",
    "### Best of both worlds:\n",
    "Our `CoreLabSession` provides session management while SageMaker SDK handles ML operations - giving you both control and convenience!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
